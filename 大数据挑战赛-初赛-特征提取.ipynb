{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD15BE576B4C4D128B3AA9EB9A9A8C76",
    "mdEditEnable": false
   },
   "source": [
    "# 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "40F26B9530E44CC49103F8FD48BE1CBA",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys, os, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "sys.path.append(\"./utils\")\n",
    "from helper import ReadCSV, Timer, ExtractFeature, ORI_TRAIN_NAMES, ORI_TEST_NAMES, AnalysisCSV, FuncMap2, FuncMap1\n",
    "import dist_utils, ngram_utils\n",
    "base_feature_save_dir = \"./outputs/features/\"\n",
    "OFFLINE = True\n",
    "test_file = \"./inputs/test.csv\"\n",
    "if OFFLINE:\n",
    "    base_prefix = \"debug_\"\n",
    "    train_file = \"./inputs/train_last_50w.csv\"\n",
    "    CHUNK_SIZE = 100000\n",
    "else:\n",
    "    base_prefix = \"online_\"\n",
    "    train_file = \"./inputs/train_last_1000w.csv\"\n",
    "    CHUNK_SIZE = 1000000\n",
    "print(OFFLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "761C585EC2274901AB7E188FED28CA82",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ! pip install  --index \"http://pypi/simple\" --trusted-host pypi fuzzywuzzy\n",
    "# ! pip install  --index \"http://pypi/simple\" --trusted-host pypi  pyemd\n",
    "# ! pip install  --index \"http://pypi/simple\" --trusted-host pypi simhash\n",
    "# ! pip install  --index \"http://pypi/simple\" --trusted-host pypi networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C6CEED2F1574088BFB7BD0B3AFA68C0",
    "mdEditEnable": false
   },
   "source": [
    "## 文本挖掘特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "C1B9B465F2234AD68651BF514CC7D8E6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "drwxr-xr-x  2 niudong  staff    64B  7 12 23:43 \u001b[1m\u001b[36m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  3 niudong  staff    96B  7 13 16:53 \u001b[1m\u001b[36m..\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "feature_save_dir = base_feature_save_dir + \"textMining/\"\n",
    "train_feature_prefix = base_prefix + \"trainTextMining\"\n",
    "test_feature_prefix = base_prefix + \"testTextMining\"\n",
    "! ls -alh ./outputs/features/textMining/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBE478C8B05D40B680CBD2AB3E3CAFF1",
    "mdEditEnable": false
   },
   "source": [
    "### 提取长度特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "E7F6EF4CF5FB44CC850D8E2DA410704C",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regen_len_feature = False\n",
    "feature_name = \"len\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "60DD46D6363B47688636727332B06FBA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_text_len(df, ngram, prefix):\n",
    "    q_list = df['query'].apply(ngram)\n",
    "    t_list = df['title'].apply(ngram)\n",
    "    with Timer(\"extract length\"):\n",
    "        q_len = FuncMap2(len, q_list)\n",
    "        t_len = FuncMap2(len, t_list)\n",
    "        df['%s_qLen' % prefix] = q_len\n",
    "        df['%s_tLen' % prefix] = t_len\n",
    "        df['%s_qtLenRatio'%prefix] = FuncMap2(lambda a, b: a/b, q_len, t_len)\n",
    "        df['%s_tqLenRatio'%prefix] = FuncMap2(lambda a, b: a/b, t_len, q_len)\n",
    "        df['%s_qtDiff'%prefix] = FuncMap2(lambda a, b: abs(a-b), q_len, t_len)\n",
    "        df['%s_qtMax'%prefix] = FuncMap2(lambda a, b: max(a, b), q_len, t_len)\n",
    "        df['%s_qtMin'%prefix] = FuncMap2(lambda a, b: min(a, b), q_len, t_len)\n",
    "        df['%s_qtAvg'%prefix] = FuncMap2(lambda a, b: (a+b)/2, q_len, t_len)\n",
    "        del q_len, t_len, q_list, t_list\n",
    "        gc.collect()\n",
    "    return df\n",
    "def process_text_len(df, save_dir, prefix, feature_name):\n",
    "    df = run_text_len(df, ngram_utils.unichars, '%s_%s' % (prefix, 'unichars'))\n",
    "    df = run_text_len(df, ngram_utils.unigrams, '%s_%s' % (prefix, 'unigrams'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CBD06EEC5BBD4004B33CE283911513CF",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 提取训练数据特征\n",
    "if regen_len_feature:\n",
    "    ExtractFeature(train_file, feature_save_dir, train_feature_prefix, feature_name, process_func=process_text_len, \n",
    "        names=ORI_TRAIN_NAMES, process_chunkly=False, chunk_size=CHUNK_SIZE, \n",
    "        drop_first_cols=['query_id', 'query_title_id', 'label'], drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "B490167AEC954DE19717808444B13724",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# AnalysisCSV(\"./stage1/output/text_mining_feature/debug_trainTextMining_len.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5CEF3CB2AEB44E038AD28D5EBC235D88",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not OFFLINE and regen_len_feature:\n",
    "    ExtractFeature(test_file, feature_save_dir, test_feature_prefix, feature_name, process_func=process_text_len, \n",
    "    names=ORI_TEST_NAMES, process_chunkly=False, chunk_size=CHUNK_SIZE, \n",
    "    drop_first_cols=['query_id', 'query_title_id'], drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AE102779A0EE42E8A4128D44605553E5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 989 µs\n"
     ]
    }
   ],
   "source": [
    "# AnalysisCSV(\"./stage1/output/text_mining_feature/debug_train_text_mining_feature_len.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B76B2EE7B7C45258B815B1BB4F0C1C5",
    "mdEditEnable": false
   },
   "source": [
    "### 提取字符编辑距离特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "E68A493A8B244468942CF4859F548D1E",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "regen_edit_sim_feature = False\n",
    "feature_name = \"editSim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "66D053F91FB442728158DCA5C25ABE62",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "similar_arr = [\n",
    "    # https://www.jb51.net/article/98449.htm;\n",
    "    # http://www.coli.uni-saarland.de/courses/LT1/2011/slides/Python-Levenshtein.html#Levenshtein-inverse\n",
    "    Levenshtein.distance,\n",
    "    Levenshtein.jaro,\n",
    "    Levenshtein.jaro_winkler,\n",
    "    # https://blog.csdn.net/qq_43174128/article/details/82595317\n",
    "    fuzz.ratio,\n",
    "    fuzz.partial_ratio,\n",
    "    fuzz.token_sort_ratio,\n",
    "    fuzz.partial_token_sort_ratio,\n",
    "    fuzz.token_set_ratio\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6FDAF1DCC57F4B958F5DB383FC836159",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_edit_sim_feature(df, prefix):\n",
    "    q_list = df['query']\n",
    "    t_list = df['title']\n",
    "    with Timer(\"extract edit sim\"):\n",
    "        for _ in similar_arr:\n",
    "            name = _.__name__\n",
    "            with Timer(\"cal {} sim\".format(name)):\n",
    "                df[\"{}_{}\".format(prefix, name)] = FuncMap2(_, q_list, t_list)\n",
    "        del q_list, t_list\n",
    "        gc.collect()\n",
    "    return df\n",
    "def process_edit_sim_feature(df, save_dir, prefix, feature_name):\n",
    "    df = run_edit_sim_feature(df, '%s_%s' % (prefix, 'text'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "E20960C2CED0432089F1FE9D935EAB72",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_edit_sim_feature:\n",
    "    ExtractFeature(train_file, feature_save_dir, train_feature_prefix, feature_name, \n",
    "        process_func=process_edit_sim_feature, names=ORI_TRAIN_NAMES, process_chunkly=False, \n",
    "        chunk_size=CHUNK_SIZE, drop_first_cols=['query_id', 'query_title_id', 'label'], \n",
    "        drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1314AD4E4E794EDB9BC13AA8140CD441",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not OFFLINE and regen_edit_sim_feature:\n",
    "    ExtractFeature(test_file, feature_save_dir, test_feature_prefix, feature_name, \n",
    "        process_func=process_edit_sim_feature, names=ORI_TEST_NAMES, process_chunkly=False, \n",
    "        chunk_size=CHUNK_SIZE, drop_first_cols=['query_id', 'query_title_id'], \n",
    "        drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DDCC4D135FF46D086F1D9D77D7C29F3",
    "mdEditEnable": false
   },
   "source": [
    "### 提取集合相似度特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "A9F24C2040384D4BBE2B355D542EA06F",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "regen_ngram_sim_feature = False\n",
    "feature_name = \"ngramSim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "C8534A0F74A54363AE989AD3FBED6EC9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "similar_arr = [\n",
    "    # 集合的交集关系\n",
    "    # 可以操作字符串，如\"abc\"；也可以操作字符数组，如['a', 'b', 'c']\n",
    "    dist_utils.dice_ratio,\n",
    "    dist_utils.jaccard_ratio,\n",
    "    dist_utils.edit_seq_ratio,\n",
    "    dist_utils.edit_set_ratio,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "F06C42D8A5B649B2868E541787C39840",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_ngram_similarity(df, ngram, prefix):\n",
    "    q_list = df['query'].apply(ngram)\n",
    "    t_list = df['title'].apply(ngram)\n",
    "    with Timer(\"extract ngram sim\"):\n",
    "        for _ in similar_arr:\n",
    "            name = _.__name__\n",
    "            with Timer(\"cal {} sim\".format(name)):\n",
    "                df[\"{}_{}\".format(prefix, name)] = FuncMap2(_, q_list, t_list)\n",
    "        del q_list, t_list\n",
    "        gc.collect()\n",
    "    return df\n",
    "\n",
    "def process_ngram_similarity(df, save_dir, prefix, feature_name):\n",
    "    df = run_ngram_similarity(df, ngram_utils.unichars, '%s_%s' % (prefix, 'unichars'))\n",
    "    df = run_ngram_similarity(df, ngram_utils.bichars, '%s_%s' % (prefix, 'bichars'))\n",
    "    df = run_ngram_similarity(df, ngram_utils.trichars, '%s_%s' % (prefix, 'trichars'))\n",
    "    df = run_ngram_similarity(df, ngram_utils.unigrams, '%s_%s' % (prefix, 'unigrams'))\n",
    "    df = run_ngram_similarity(df, ngram_utils.bigrams, '%s_%s' % (prefix, 'bigrams'))\n",
    "    df = run_ngram_similarity(df, ngram_utils.trigrams, '%s_%s' % (prefix, 'trigrams'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "B042F9B774C24DFD848620F742E833B1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 提取训练数据特征\n",
    "if regen_ngram_sim_feature:\n",
    "    ExtractFeature(train_file, feature_save_dir, train_feature_prefix, feature_name, \n",
    "        process_func=process_ngram_similarity, names=ORI_TRAIN_NAMES, process_chunkly=False, \n",
    "        chunk_size=CHUNK_SIZE, drop_first_cols=['query_id', 'query_title_id', 'label'], \n",
    "        drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "FBBBDAAE926D49038C9C5E824014F74C",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# AnalysisCSV(\"./stage1/output/text_mining_feature/debug_trainTextMining_ngramSim.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "F6447FEAEFAC431C89BEB6ECDD6208BF",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not OFFLINE and regen_ngram_sim_feature:\n",
    "    ExtractFeature(test_file, feature_save_dir, test_feature_prefix, feature_name, \n",
    "    process_func=process_ngram_similarity, names=ORI_TEST_NAMES, process_chunkly=False, \n",
    "    chunk_size=CHUNK_SIZE, drop_first_cols=['query_id', 'query_title_id'], \n",
    "    drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34FF8ADD40D042E38EAA9C8DC3357D5C",
    "mdEditEnable": false
   },
   "source": [
    "### SimHash特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "EC370B7E7BD245158D2734F1EC4630EB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from simhash import Simhash\n",
    "# https://github.com/cjauvin/simhash/blob/master/tests/test_simhash.py\n",
    "# https://leons.im/posts/a-python-implementation-of-simhash-algorithm/\n",
    "# 这个不错: http://yanyiwu.com/work/2014/01/30/simhash-shi-xian-xiang-jie.html\n",
    "tfidf_model_file = \"./outputs/models/online_tfidf_model.bin\"\n",
    "regen_simhash = False\n",
    "feature_name = \"ngramHashsim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "B904E065A5484A0EBC4C2F9A27C9384D",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_ngram_simhash(df, prefix):\n",
    "    with Timer(\"cal tfidf vec\"):\n",
    "        with open(tfidf_model_file, \"rb\") as ff:\n",
    "            tfidf_model = pickle.load(ff)\n",
    "            voc = {i:w for w, i in tfidf_model.vocabulary_.items()}\n",
    "        q_vecs = tfidf_model.transform(df['query'])\n",
    "        t_vecs = tfidf_model.transform(df['title'])\n",
    "    with Timer(\"cal sim hash\"):\n",
    "        q_simhash = [\n",
    "            Simhash(\n",
    "                zip([voc[j] for j in Di.indices], Di.data)\n",
    "            )\n",
    "            for Di in q_vecs\n",
    "        ]\n",
    "        t_simhash = [\n",
    "            Simhash(\n",
    "                zip([voc[j] for j in Di.indices], Di.data)\n",
    "            )\n",
    "            for Di in t_vecs\n",
    "        ]\n",
    "    with Timer(\"cal hash sim\"):\n",
    "        df[\"{}Simhash\".format(prefix)] = FuncMap2(lambda x,y: x.distance(y), q_simhash, t_simhash)\n",
    "        \n",
    "    del q_vecs, t_vecs, q_simhash, t_simhash, tfidf_model, voc\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_ngram_simhash(df, save_dir, prefix, feature_name):\n",
    "    df = run_ngram_simhash(df, prefix)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "C3DF38230E854BE18E69868A0606675D",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 提取训练数据特征\n",
    "if regen_simhash:\n",
    "    ExtractFeature(train_file, feature_save_dir, train_feature_prefix, feature_name, \n",
    "                   process_func=process_ngram_simhash, names=ORI_TRAIN_NAMES, process_chunkly=False, \n",
    "                   chunk_size=CHUNK_SIZE, drop_first_cols=['query_id', 'query_title_id', 'label'], \n",
    "                   drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67AD058D138E43BAA3781D9CC9CCC2E2",
    "mdEditEnable": false
   },
   "source": [
    "### 最长公共子串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "8271DCC453B24D2EB8F0BF3667D464E7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'', '23', '123', '2', '3', '1', '13', '12'}\n"
     ]
    }
   ],
   "source": [
    "import py_common_subseq\n",
    "regen_subseq = False\n",
    "feature_name = \"commonSeq\"\n",
    "print(py_common_subseq.find_common_subsequences(\"123\", \"13423\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "2B000BEFCADD436AB632AEF50D087C27",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_ngram_subseq(df, ngram, prefix):\n",
    "    query = df[\"query\"].apply(ngram)\n",
    "    title = df[\"title\"].apply(ngram)\n",
    "    with Timer(\"cal sub seq\"):\n",
    "        df[\"{}LCSValue\".format(prefix)] = FuncMap2(lambda x, y:py_common_subseq.find_common_subsequences(x, y),query, title)\n",
    "        del query, title\n",
    "        gc.collect()\n",
    "    return df\n",
    "\n",
    "def process_ngram_subseq(df, save_dir, prefix, feature_name):\n",
    "    df = run_ngram_subseq(df, ngram_utils.unichars, '%s_%s' % (prefix, 'unichars'))\n",
    "    df = run_ngram_subseq(df, ngram_utils.bichars, '%s_%s' % (prefix, 'bichars'))\n",
    "    df = run_ngram_subseq(df, ngram_utils.trichars, '%s_%s' % (prefix, 'trichars'))\n",
    "    df = run_ngram_subseq(df, ngram_utils.unigrams, '%s_%s' % (prefix, 'unigrams'))\n",
    "    df = run_ngram_subseq(df, ngram_utils.bigrams, '%s_%s' % (prefix, 'bigrams'))\n",
    "    df = run_ngram_subseq(df, ngram_utils.trigrams, '%s_%s' % (prefix, 'trigrams'))\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "6A0AB78AA49747CC95B2F27A078CF871",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 提取训练数据特征\n",
    "if regen_subseq:\n",
    "    ExtractFeature(train_file, feature_save_dir, train_feature_prefix, feature_name, \n",
    "        process_func=process_ngram_subseq, names=ORI_TRAIN_NAMES, process_chunkly=False, chunk_size=CHUNK_SIZE, \n",
    "        drop_first_cols=['query_id', 'query_title_id', 'label'], drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "29C92F1B1B794E7194CD02D53FC5BFC1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 提取训练数据特征\n",
    "if not OFFLINE and regen_subseq:\n",
    "    ExtractFeature(test_file, feature_save_dir, test_feature_prefix, feature_name, \n",
    "        process_func=process_ngram_subseq, names=ORI_TEST_NAMES, process_chunkly=False, chunk_size=CHUNK_SIZE, \n",
    "        drop_first_cols=['query_id', 'query_title_id', 'label'], drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15E7A63098994D708D5CD5744E814A33",
    "mdEditEnable": false
   },
   "source": [
    "## 向量空间特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "BBE2052F8A5D48EBAFF63A4DC5CEE6AB",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxr-xr-x  2 niudong  staff    64B  7 13 18:32 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 niudong  staff   128B  7 13 18:32 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "./oututs/models/tfidf_word_ngram14.bin\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "feature_save_dir = base_feature_save_dir + \"vectorSpace/\"\n",
    "train_feature_prefix = base_prefix + \"trainVectorSpace\"\n",
    "test_feature_prefix = base_prefix + \"testVectorSpace\"\n",
    "ngram_range, ngram_level = (1, 4), \"word\"\n",
    "tfidf_model_file = \"./oututs/models/tfidf_{}_ngram{}{}.bin\".format(\n",
    "    ngram_level, ngram_range[0], ngram_range[1])\n",
    "! ls -alh ./outputs/features/vectorSpace/\n",
    "print(tfidf_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AD8B31E0B0D24055ABB2462F7C9E0D7A",
    "mdEditEnable": false
   },
   "source": [
    "### 训练tfidf模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "DAE6BBDE89184CC89B739475558CE7AD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regen_tfidf_model = False\n",
    "tfidf_train_file = \"./inputs/train_last_1000w.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "5B1C90A46AE844FA824D86B50A63650C",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_tfidf(df, prefix, savefile):\n",
    "    word_tfidf = TfidfVectorizer(norm=\"l2\",  # 'l2 norm' can cal similarty\n",
    "                                    strip_accents=\"unicode\",\n",
    "                                    analyzer=ngram_level,\n",
    "                                    ngram_range=ngram_range,\n",
    "                                    use_idf=True,\n",
    "                                    smooth_idf=True,\n",
    "                                    sublinear_tf=True, min_df=5, max_df=0.9)\n",
    "    new_query = df[\"query\"].unique()\n",
    "    new_title = df[\"title\"].values\n",
    "    corpus = np.concatenate([new_query, new_title])\n",
    "    del new_query, new_title, df\n",
    "    gc.collect()\n",
    "    with Timer(\"train tfidf\"):\n",
    "        word_tfidf.fit(corpus)\n",
    "        with open(tfidf_model_file, \"wb\") as f:\n",
    "            pickle.dump(word_tfidf, f)\n",
    "            print(\"Model dumped to {}\".format(tfidf_model_file))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "481C9684448446EDB76E03D030B50B3C",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'extract tfidf feature' block...\n",
      "Mem. usage decreased to 76.29 Mb (0.0% reduction)\n",
      "----->Started 'tf-idf fit' block...\n"
     ]
    }
   ],
   "source": [
    "# 提取训练数据特征\n",
    "if regen_tfidf_model:\n",
    "    ExtractFeature(tfidf_train_file, feature_save_dir, train_feature_prefix, None, \n",
    "        process_func=train_tfidf, names=ORI_TRAIN_NAMES, process_chunkly=False, \n",
    "        chunk_size=CHUNK_SIZE, drop_first_cols=['query_id', 'query_title_id', 'label'], \n",
    "        drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75C8C3F4EF534B12ADC50BF155A934BE",
    "mdEditEnable": false
   },
   "source": [
    "### tfidf向量相似性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "FDBEAC0E2434445CBE2B3784BBFA4F56",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./oututs/models/tfidf_word_ngram14.bin\n"
     ]
    }
   ],
   "source": [
    "regen_tfidf_vec_sim = False\n",
    "feature_name = \"tfidfVecSim\"\n",
    "print(tfidf_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "691C30D600E84925BEB68A9F656939A2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise\n",
    "from sklearn.metrics.pairwise import *\n",
    "similar_dis = [\n",
    "    paired_cosine_distances,\n",
    "    paired_euclidean_distances,\n",
    "    paired_manhattan_distances\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "1418FD4036F544B58A4A0532E2EB875C",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_tfidf_vec_sim(df, save_dir, prefix, feature_name):\n",
    "    with Timer(\"cal tfidf vec\"):\n",
    "        with open(tfidf_model_file, \"rb\") as ff:\n",
    "            tfidf_model = pickle.load(ff)\n",
    "        q_vec = tfidf_model.transform(df[\"query\"])\n",
    "        t_vec = tfidf_model.transform(df[\"title\"])\n",
    "    with Timer(\"cal tfidf sim\"):\n",
    "        for _ in similar_dis:\n",
    "            name = _.__name__\n",
    "            with Timer(\"cal {} sim\".format(name)):\n",
    "                df[\"{}_{}\".format(prefix, name)] = _(q_vec, t_vec)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "33FAACC8B97241C2815D36BD0E3FD903",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_tfidf_vec_sim:\n",
    "    ExtractFeature(train_file, feature_save_dir, train_feature_prefix, feature_name, \n",
    "        process_func=process_tfidf_vec_sim, names=ORI_TRAIN_NAMES,\n",
    "        process_chunkly=False, chunk_size=CHUNK_SIZE, \n",
    "        drop_first_cols=['query_id', 'query_title_id', 'label'], \n",
    "        drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "6275F3B4888E4E7C8D265DCC3A2AEC39",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not OFFLINE and regen_tfidf_vec_sim:\n",
    "    ExtractFeature(test_file, feature_save_dir, test_feature_prefix, feature_name, \n",
    "        process_func=process_tfidf_vec_sim, names=ORI_TEST_NAMES,\n",
    "        process_chunkly=False, chunk_size=CHUNK_SIZE, \n",
    "        drop_first_cols=['query_id', 'query_title_id', 'label'], \n",
    "        drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "571373829A4B43CFA173AEDB3BCC8EAF",
    "mdEditEnable": false
   },
   "source": [
    "### tfidf长度特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "74796697CD964CE596198B4451A5D8A8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./oututs/models/tfidf_word_ngram14.bin\n"
     ]
    }
   ],
   "source": [
    "from np_utils import try_divide\n",
    "regen_tfidf_vec_len = False\n",
    "import scipy, numpy\n",
    "feature_name = \"tfidfVecLen\"\n",
    "print(tfidf_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "31EBDDCAB43D45A28B1F7B7F827F1D28",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_tfidf_vec_len(df, save_dir, prefix, feature_name):\n",
    "    with Timer(\"cal tfidf vec\"):\n",
    "        with open(tfidf_model_file, \"rb\") as ff:\n",
    "            tfidf_model = pickle.load(ff)\n",
    "        q_vec_len = np.array(tfidf_model.transform(df[\"query\"]).sum(1)).squeeze()\n",
    "        t_vec_len = np.array(tfidf_model.transform(df[\"title\"]).sum(1)).squeeze()\n",
    "    with Timer(\"cal tfidf len\"):\n",
    "        df['%s_QLen' % prefix] = q_vec_len\n",
    "        df['%s_TLen' % prefix] = t_vec_len\n",
    "        df['%s_QTLenRatio'%prefix] = FuncMap2(lambda a, b: try_divide(a, b), q_vec_len, t_vec_len)\n",
    "        df['%s_TQLenRatio'%prefix] = FuncMap2(lambda a, b: try_divide(b, a), q_vec_len, t_vec_len)\n",
    "        df['%s_QTDiff'%prefix] = FuncMap2(lambda a, b: abs(a-b), q_vec_len, t_vec_len)\n",
    "        df['%s_QTMax'%prefix] = FuncMap2(lambda a, b: max(a, b), q_vec_len, t_vec_len)\n",
    "        df['%s_QTMin'%prefix] = FuncMap2(lambda a, b: min(a, b), q_vec_len, t_vec_len)\n",
    "        df['%s_QTAvg'%prefix] = FuncMap2(lambda a, b: (a+b)/2, q_vec_len, t_vec_len)\n",
    "        df['%s_QTMulti'%prefix] = FuncMap2(lambda a, b: a*b, q_vec_len, t_vec_len)\n",
    "        # df['%s_QSquare2'%prefix] = numpy.square(q_vec_len)\n",
    "        # df['%s_TSquare2'%prefix] = numpy.square(t_vec_len)\n",
    "        # df['%s_QSqrt'%prefix] = numpy.sqrt(q_vec_len)\n",
    "        # df['%s_TSqrt'%prefix] = numpy.sqrt(t_vec_len)\n",
    "        # df['%s_QLog'%prefix] = numpy.log(q_vec_len)\n",
    "        # df['%s_TLog'%prefix] = numpy.log(t_vec_len)\n",
    "        # p_corr = scipy.stats.pearsonr(q_vec_len, t_vec_len)[0]\n",
    "        # df['%s_PCorr'%prefix] = [p_corr] * len(q_vec_len)\n",
    "        del q_vec_len, t_vec_len\n",
    "        gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "6669F53B4AFC4B5F84762938D65BFE97",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_tfidf_vec_len:\n",
    "    ExtractFeature(train_file, feature_save_dir, train_feature_prefix, feature_name, \n",
    "        process_func=process_tfidf_vec_len, names=ORI_TRAIN_NAMES, \n",
    "        process_chunkly=False, chunk_size=CHUNK_SIZE, \n",
    "        drop_first_cols=['query_id', 'query_title_id', 'label'], \n",
    "        drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "6B239555BB7A4AAE9FC1E97081FA1635",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not OFFLINE and regen_tfidf_vec_len:\n",
    "    ExtractFeature(test_ file, feature_save_dir, test_feature_prefix, feature_name, \n",
    "        process_func=process_tfidf_vec_len, names=ORI_TEST_NAMES, \n",
    "        process_chunkly=False, chunk_size=CHUNK_SIZE, \n",
    "        drop_first_cols=['query_id', 'query_title_id'], \n",
    "        drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5985318916B1435D8687DE64F698F4E5",
    "mdEditEnable": false
   },
   "source": [
    "## 提取word2vec特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "07E9877B444D484E93CB0BC611F297FE",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "drwxr-xr-x  2 niudong  staff    64B  7 13 20:41 \u001b[1m\u001b[36m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  5 niudong  staff   160B  7 13 20:41 \u001b[1m\u001b[36m..\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from scipy.stats import skew, kurtosis\n",
    "word2vec_model_file = \"./outputs/models/word2vec.kv\"\n",
    "ngram_range, ngram_level = (1, 4), \"word\"\n",
    "tfidf_model_file = \"./oututs/models/tfidf_{}_ngram{}{}.bin\".format(\n",
    "    ngram_level, ngram_range[0], ngram_range[1])\n",
    "feature_save_dir = base_feature_save_dir + \"word2vec/\"\n",
    "train_feature_prefix = base_prefix + \"trainWord2vec\"\n",
    "test_feature_prefix = base_prefix + \"testWord2vec\"\n",
    "! ls -alh ./outputs/features/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "76A3616FFE05428F83B7C636DE6CE552",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sent2vec1(text):\n",
    "    # 计算embedding均值\n",
    "    global word2vec_model, tfidf_idf, tfidf_vocab\n",
    "    words = text.split()\n",
    "    words_num = len(words)\n",
    "    return np.nan_to_num(\n",
    "          np.array([np.array(word2vec_model[w])*(\n",
    "             (1/words_num) * tfidf_idf[tfidf_vocab[w]]\n",
    "             ) for w in words \n",
    "             if w in word2vec_model and w in tfidf_vocab] or [0.0] * 200\n",
    "          ).sum(axis=0)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "CDECCF235D094B0182DED24769FAF3F0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sent2vec(text):\n",
    "    # 计算embedding均值\n",
    "    global word2vec_model\n",
    "    return np.nan_to_num(\n",
    "          np.array([word2vec_model[w] for w in text.split() if w in word2vec_model] or [0.0] * 200\n",
    "          ).mean(axis=0)\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00019D0B86C54FAC8E9CCDEF35F7E344",
    "mdEditEnable": false,
    "scrolled": false
   },
   "source": [
    "### 生成word2vec句子相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "FE844A39B4D441F588A8C50DA105123E",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# word2vec to sent: https://www.zhihu.com/question/29978268\n",
    "regen_word2vec_sen_vec_sim = False\n",
    "process_word2vec_sen_vec_mode = \"avg\"  # \"avg\" or \"tfidf\"\n",
    "feature_name = \"senVecSim{}\".format(process_word2vec_sen_vec_mode.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "E81C29274AFD4D838D3D2455266D6A3C",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "similar_dis = [\n",
    "    dist_utils.cosine_distance, \n",
    "    dist_utils.jaccard_distance,\n",
    "    dist_utils.braycurtis_distance,\n",
    "    dist_utils.canberra_distance,\n",
    "    dist_utils.cityblock_distance,\n",
    "    dist_utils.euclidean_distance,\n",
    "    dist_utils.minkowski_distance\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "8DDC1915D9AF4A0692DDC47880088AC9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_word2vec_sen_vec_sim:\n",
    "    word2vec_model = KeyedVectors.load(word2vec_model_file, mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "1C8A8131A3B24C538E2C5A29DE4A3873",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_word2vec_sen_vec_sim and process_word2vec_sen_vec_mode == \"tfidf\":\n",
    "    with open(tfidf_model_file, \"rb\") as ff:\n",
    "        tfidf_model = pickle.load(ff)\n",
    "    tfidf_idf = tfidf_model.idf_\n",
    "    tfidf_vocab = tfidf_model.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "FEF992DC1226417590B0BA329DC801B1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_word2vec_sen_vec(df, save_dir, prefix, feature_name):\n",
    "    with Timer(\"cal sent vec\"):\n",
    "        if process_word2vec_sen_vec_mode == \"avg\":\n",
    "            q_sen_vec = list(map(sent2vec, df[\"query\"]))\n",
    "            t_sen_vec = list(map(sent2vec, df[\"title\"]))\n",
    "        elif process_word2vec_sen_vec_mode == \"tfidf\":\n",
    "            q_sen_vec = list(map(sent2vec1, df[\"query\"]))\n",
    "            t_sen_vec = list(map(sent2vec1, df[\"title\"]))\n",
    "        for _ in similar_dis:\n",
    "            name = _.__name__\n",
    "            with Timer(\"cal {} sim\".format(name)):\n",
    "                df[\"{}_{}\".format(prefix, name)] = FuncMap2(_, q_sen_vec, t_sen_vec)\n",
    "        del q_sen_vec, t_sen_vec\n",
    "        gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "EAA93E0DDE01465C837D79A893136B06",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_word2vec_sen_vec_sim:\n",
    "    ExtractFeature(train_file, feature_save_dir, train_feature_prefix, feature_name, \n",
    "        process_func=process_word2vec_sen_vec, names=ORI_TRAIN_NAMES,\n",
    "        process_chunkly=False, chunk_size=CHUNK_SIZE, \n",
    "        drop_first_cols=['query_id', 'query_title_id', 'label'], \n",
    "        drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "ADCABA33318B4733833905C11E2B196E",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 提取训练数据特征\n",
    "if regen_word2vec_sen_vec_sim and not OFFLINE:\n",
    "    ExtractFeature(test_file, feature_save_dir, test_feature_prefix, feature_name, \n",
    "        process_func=process_word2vec_sen_vec, names=ORI_TEST_NAMES, \n",
    "        process_chunkly=False, chunk_size=CHUNK_SIZE, \n",
    "        drop_first_cols=['query_id', 'query_title_id'], \n",
    "        drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8BC5809EC534E3680559FD378A629F1",
    "mdEditEnable": false
   },
   "source": [
    "### word2vec其他特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "E2F0C59AC64C48FB88C1DD00F8E6792F",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word2vec_model_norm = None\n",
    "regen_word2vec_sen_vec_other = False\n",
    "process_word2vec_sen_vec_mode = \"avg\"\n",
    "feature_name = \"senVecOther{}\".format(process_word2vec_sen_vec_mode.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "335A3AC4342F45A8B6409E565BD59C0F",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def norm_wmd(s1, s2):\n",
    "    global word2vec_model_norm\n",
    "    dis = np.nan_to_num(word2vec_model_norm.wmdistance(s1, s2))\n",
    "    return dis if dis<100 else 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "A75CAA83ECB7483A85E18278583B7282",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def wmd(s1, s2):\n",
    "    global word2vec_model\n",
    "    dis = np.nan_to_num(word2vec_model.wmdistance(s1, s2))\n",
    "    return dis if dis<100 else 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "D37E5667CBE84D61BD94A54807D4AC59",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_word2vec_sen_vec_wmd(df, ngram, prefix, name):\n",
    "    q_list = df['query'].apply(ngram)\n",
    "    t_list = df['title'].apply(ngram)\n",
    "    df['%s_wmd%s' % (prefix, name)] = FuncMap2(wmd, q_list, t_list)\n",
    "    df['%s_normWmd%s' % (prefix, name)] = FuncMap2(norm_wmd, q_list, t_list)\n",
    "    del q_list, t_list\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "DFD1EBFA93D54C63B798ACFD526C3F8C",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_word2vec_sen_vec_sk(df, func, q_sen_vec, t_sen_vec, prefix, name):\n",
    "    q_value = FuncMap1(func, q_sen_vec)\n",
    "    t_value = FuncMap1(func, t_sen_vec)\n",
    "    df['%s_q%s' % (prefix, name)] = q_value\n",
    "    df['%s_t%s' % (prefix, name)] = t_value\n",
    "    df['%s_%sDiff'%(prefix, name)] = FuncMap2(lambda a, b: abs(a-b), q_value, t_value)\n",
    "    df['%s_%sMax'%(prefix, name)] = FuncMap2(lambda a, b: max(a, b), q_value, t_value)\n",
    "    df['%s_%sMin'%(prefix, name)] = FuncMap2(lambda a, b: min(a, b), q_value, t_value)\n",
    "    df['%s_%sAvg'%(prefix, name)] = FuncMap2(lambda a, b: (a+b)/2, q_value, t_value)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "3CC24C2852BD4FCFAF855DA809F7A497",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_word2vec_sen_vec_other(df, save_dir, prefix, feature_name):\n",
    "    \n",
    "    global word2vec_model_norm, word2vec_model_file\n",
    "    word2vec_model_norm = KeyedVectors.load(word2vec_model_file)\n",
    "    word2vec_model_norm.init_sims(replace=True)\n",
    "    \n",
    "    # with Timer(\"cal uni-wmd dis\"):\n",
    "    #     df = run_word2vec_sen_vec_wmd(df, ngram_utils.unigrams, prefix, \"Unigrams\")\n",
    "    # with Timer(\"cal bi-wmd dis\"):\n",
    "    #     df = run_word2vec_sen_vec_wmd(df, ngram_utils.bigrams, prefix, \"Bigrams\")\n",
    "        \n",
    "    with Timer(\"cal sent vec\"):\n",
    "        global process_word2vec_sen_vec_mode\n",
    "        if process_word2vec_sen_vec_mode == \"avg\":\n",
    "            q_sen_vec = FuncMap1(sent2vec, df[\"query\"])\n",
    "            t_sen_vec = FuncMap1(sent2vec, df[\"title\"])\n",
    "        elif process_word2vec_sen_vec_mode == \"tfidf\":\n",
    "            q_sen_vec = FuncMap1(sent2vec1, df[\"query\"])\n",
    "            t_sen_vec = FuncMap1(sent2vec1, df[\"title\"])\n",
    "    \n",
    "    with Timer(\"cal skew\"):\n",
    "        df = run_word2vec_sen_vec_sk(df, skew, q_sen_vec, t_sen_vec, prefix, \"Skew\")\n",
    "    with Timer(\"cal kurtosis\"):\n",
    "        df = run_word2vec_sen_vec_sk(df, kurtosis, q_sen_vec, t_sen_vec, prefix, \"Kurtosis\")\n",
    "        \n",
    "    del q_sen_vec, t_sen_vec\n",
    "    gc.collect()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "DBD6C46BAC0345B68376EC52A2203471",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_word2vec_sen_vec_other:\n",
    "    ExtractFeature(train_file, feature_save_dir, train_feature_prefix, feature_name,\n",
    "        process_func=process_word2vec_sen_vec_other, names=ORI_TRAIN_NAMES,\n",
    "        process_chunkly=True, chunk_size=CHUNK_SIZE, \n",
    "        drop_first_cols=['query_id', 'query_title_id', 'label'], \n",
    "        drop_last_cols=['query', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "152557B408BF4CEEA6C5FE653C6CD8D6",
    "mdEditEnable": false
   },
   "source": [
    "## Magic特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "4C2E0149E5FE465EBCB250DEAD7FB466",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: ./outputs/magic: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "feature_save_dir = base_feature_save_dir + \"magic/\"\n",
    "train_feature_prefix = base_prefix + \"trainMagic\"\n",
    "test_feature_prefix = base_prefix + \"testMagic\"\n",
    "! ls -alh ./outputs/magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "415BAFF0C7C146A587DB4BA7DD677742",
    "mdEditEnable": false
   },
   "source": [
    "### 提取query频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "82F529A01DFF4B1483991308775A9972",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regen_query_freq = False\n",
    "train_query_freq = train_feature_prefix + \"_queryFreq\"\n",
    "train_freq_save_file = feature_save_dir+train_query_freq+\".csv.gz\"\n",
    "if not OFFLINE:\n",
    "    test_query_freq = test_feature_prefix + \"_queryFreq\"\n",
    "    test_freq_save_file = feature_save_dir+test_query_freq+\".csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "52FD73F8CA96480F8289B30CE1207864",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ExtractFreq(sourcefile, savefile, names, group_by, sort_col, feature_name):\n",
    "    df = ReadCSV(sourcefile, names=names, iterator=False)\n",
    "    grouped = df.groupby(group_by, as_index=False)[sort_col].count()\n",
    "    res = np.concatenate(list(map(lambda x: [x]*x, grouped[sort_col])))\n",
    "    tmp = pd.DataFrame({\n",
    "        feature_name: res\n",
    "    })\n",
    "    tmp.to_csv(savefile, compression=\"gzip\", index=None)\n",
    "    print(\"Freq feature saved to {}\".format(savefile))\n",
    "    del df, grouped, res, tmp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "3DC627A6B95E47B8AF09E6D22B442777",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_query_freq:\n",
    "    ExtractFreq(train_file, \n",
    "        train_freq_save_file, \n",
    "        ORI_TRAIN_NAMES, \n",
    "        \"query_id\",\n",
    "        \"query_title_id\",\n",
    "        \"query_freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "B64F181229BD4E02B1D613A1F334D5E6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_query_freq and not OFFLINE:\n",
    "    ExtractFreq(test_file, \n",
    "        test_freq_save_file, \n",
    "        ORI_TEST_NAMES, \n",
    "        \"query_id\",\n",
    "        \"query_title_id\",\n",
    "        \"query_freq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFF2D03C630F454E8DD727509F512A40",
    "mdEditEnable": false,
    "scrolled": false
   },
   "source": [
    "### 提取差集Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "07FF17550E1741328E64E5A1D1CB3523",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regen_diff_sim = False\n",
    "feature_name = \"diffSetSim\"\n",
    "similar_arr = [\n",
    "    dist_utils.dice_ratio,\n",
    "    dist_utils.jaccard_ratio,\n",
    "    dist_utils.edit_seq_ratio,\n",
    "    dist_utils.edit_set_ratio,\n",
    "]\n",
    "diffSetTitle_baseDir = feature_save_dir + feature_name + \"Title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "6EDBC7C64C1344E881BC1E5083EAD3D1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def findDiff(arr, ngram):\n",
    "    set_list = FuncMap1(lambda x: set(ngram(x)), arr)\n",
    "    res = []\n",
    "    for i, _ in enumerate(set_list):\n",
    "        result = _\n",
    "        for __ in set_list:\n",
    "            if _ != __:\n",
    "                result = result.difference(__)\n",
    "        res.append(list(result))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "005E57B7CEE743A5B02245F3E625C429",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def GetDiffSet(query_id, title, ngram):\n",
    "    tmp, res = [], []\n",
    "    for i in range(len(query_id)):\n",
    "        if i == 0:\n",
    "            tmp.append(title[i])\n",
    "        else:\n",
    "            if query_id[i] == query_id[i-1]:\n",
    "                tmp.append(title[i])\n",
    "            else:\n",
    "                res.append(findDiff(tmp, ngram))\n",
    "                tmp = [title[i]]\n",
    "    res.append(findDiff(tmp, ngram))\n",
    "    result = []\n",
    "    for _ in res:\n",
    "        result.extend(_)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "82F34AF6CED54AD1816859258246BA3C",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_diff_set_sim(df, ngram, prefix, name):\n",
    "    \n",
    "    with Timer(\"cal diff set\"):\n",
    "        t_savefile = diffSetTitle_baseDir + name +\".npy\"\n",
    "        q_list = df['query'].apply(lambda x: set(ngram(x)))\n",
    "        if os.path.exists(t_savefile):\n",
    "            t_list = np.load(t_savefile, allow_pickle=True)\n",
    "        else:\n",
    "            t_list = GetDiffSet(df[\"query_id\"], df[\"title\"], ngram)\n",
    "            np.save(t_savefile, t_list)\n",
    "            print(\"Title sent saved to {}\".format(t_savefile))\n",
    "\n",
    "    with Timer(\"extract diffSet sim\"):\n",
    "        for _ in similar_arr:\n",
    "            name = _.__name__\n",
    "            with Timer(\"cal {} sim\".format(name)):\n",
    "                df[\"{}_diffSetSim{}\".format(prefix, name)] = FuncMap2(_, q_list, t_list)\n",
    "    \n",
    "    del q_list, t_list\n",
    "    gc.collect()\n",
    "    return df\n",
    "    \n",
    "def process_diff_set_sim(df, save_dir, prefix, feature_name):\n",
    "    df = run_diff_set_sim(df, ngram_utils.unigrams, prefix, \"Unigram\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "53D8E51301584D7C8F33FA721597ED12",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_diff_sim:\n",
    "    ExtractFeature(train_file, feature_save_dir, train_feature_prefix, feature_name, \n",
    "        process_func=process_diff_set_sim, \n",
    "        names=ORI_TRAIN_NAMES, process_chunkly=False, chunk_size=CHUNK_SIZE, \n",
    "        drop_first_cols=['label', 'query_title_id'], drop_last_cols=['query_id', 'query', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "9FEE73DAEDD24A51A5691A873471A577",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_diff_sim and not OFFLINE:\n",
    "    ExtractFeature(test_file, feature_save_dir, test_feature_prefix, feature_name, \n",
    "        process_func=process_diff_set_sim, \n",
    "        names=ORI_TEST_NAMES, process_chunkly=False, chunk_size=CHUNK_SIZE, \n",
    "        drop_first_cols=['query_title_id'], drop_last_cols=['query_id', 'query', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "553A6881D6BF4F1490D4B4C30521704C",
    "mdEditEnable": false
   },
   "source": [
    "### 提取差集Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "regen_diffSet_len = False\n",
    "feature_name = \"diffSetLen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "06A4B0993A304A4683B8A34D545412E0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_diff_set_len(df, ngram, prefix, name):\n",
    "    with Timer(\"cal diff set\"):\n",
    "        t_savefile = diffSetTitle_baseDir + name +\".npy\"\n",
    "        q_list = df['query'].apply(lambda x: set(ngram(x)))\n",
    "        if os.path.exists(t_savefile):\n",
    "            t_list = np.load(t_savefile, allow_pickle=True)\n",
    "        else:\n",
    "            t_list = GetDiffSet(df[\"query_id\"], df[\"title\"], ngram)\n",
    "            np.save(t_savefile, t_list)\n",
    "            print(\"Title sent saved to {}\".format(t_savefile))\n",
    "    with Timer(\"extract diffSet len\"):\n",
    "        q_len = FuncMap1(len, q_list)\n",
    "        t_len = FuncMap1(len, t_list)\n",
    "        df['%s_diffQLen' % prefix] = q_len\n",
    "        df['%s_diffTLen' % prefix] = t_len\n",
    "        df['%s_diffQTLenRatio'%prefix] = FuncMap2(lambda a, b: a/b, q_len, t_len)\n",
    "        df['%s_diffTQLenRatio'%prefix] = FuncMap2(lambda a, b: b/a, q_len, t_len)\n",
    "        df['%s_diffQTDiff'%prefix] = FuncMap2(lambda a, b: abs(a-b), q_len, t_len)\n",
    "        df['%s_diffQTMax'%prefix] = FuncMap2(lambda a, b: max(a, b), q_len, t_len)\n",
    "        df['%s_diffQTMin'%prefix] = FuncMap2(lambda a, b: min(a, b), q_len, t_len)\n",
    "        df['%s_diffQTAvg'%prefix] = FuncMap2(lambda a, b: (a+b)/2, q_len, t_len)\n",
    "    del q_list, t_list, q_len, t_len\n",
    "    gc.collect()\n",
    "    return df\n",
    "    \n",
    "def process_diff_set_len(df, prefix, savefile):\n",
    "    df = run_diff_set_len(df, ngram_utils.unigrams, prefix, \"Unigram\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "69EF571B968C45C28A20008AEB4F5E65",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_diffSet_len:\n",
    "    ExtractFeature(train_file, feature_save_dir, train_feature_prefix, feature_name, \n",
    "        process_func=process_diff_set_len, \n",
    "        names=ORI_TRAIN_NAMES, process_chunkly=False, chunk_size=CHUNK_SIZE, \n",
    "        drop_first_cols=['label', 'query_title_id'], drop_last_cols=['query_id', 'query', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "A5E51AB6858D4A328F0943A72658E2C6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_diffSet_len and not OFFLINE: \n",
    "    ExtractFeature(test_file, feature_save_dir, test_feature_prefix, feature_name, \n",
    "        process_func=process_diff_set_len, \n",
    "        names=ORI_TEST_NAMES, process_chunkly=False, chunk_size=CHUNK_SIZE, \n",
    "        drop_first_cols=['label', 'query_title_id'], drop_last_cols=['query_id', 'query', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50F54926A22A41848D714A9F15C7A3EF",
    "mdEditEnable": false
   },
   "source": [
    "### 提取差集tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "9973C681F5D7410684783A39036696B4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from np_utils import try_divide\n",
    "ngram_range, ngram_level = (1, 4), \"word\"\n",
    "tfidf_model_file = \"./oututs/models/tfidf_{}_ngram{}{}.bin\".format(\n",
    "    ngram_level, ngram_range[0], ngram_range[1])\n",
    "regen_diffSet_tfidf_len = False\n",
    "feature_name = \"diffSetTfidfLen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "CED39BF2E31140129D5F4E6A0205D2AD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_diffSet_tfidf_len:\n",
    "    with open(tfidf_model_file, \"rb\") as ff:\n",
    "        tfidf_model = pickle.load(ff)\n",
    "        tfidf_vocab = tfidf_model.vocabulary_\n",
    "        tfidf_idf = tfidf_model.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "CF1C36C62C7042B080742936677D63A2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_uni_sum_tfidf(ngrams):\n",
    "    l = len(ngrams)\n",
    "    return sum([tfidf_idf[tfidf_vocab[_]]/l for _ in ngrams if _ in tfidf_vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "B3F0C9A3CCD3497C82A1C50FC274C73E",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_diff_set_tfidf_len(df, ngram, prefix, savefile):\n",
    "    with Timer(\"cal diff set\"):\n",
    "        t_savefile = diffSetTitle_baseDir + name +\".npy\"\n",
    "        q_list = df['query'].apply(lambda x: set(ngram(x)))\n",
    "        if os.path.exists(t_savefile):\n",
    "            t_list = np.load(t_savefile, allow_pickle=True)\n",
    "        else:\n",
    "            t_list = GetDiffSet(df[\"query_id\"], df[\"title\"], ngram)\n",
    "            np.save(t_savefile, t_list)\n",
    "            print(\"Title sent saved to {}\".format(t_savefile))\n",
    "    with Timer(\"extract similarity\"):\n",
    "        t_vec_len = list(map(get_uni_sum_tfidf, t_list))\n",
    "        q_vec_len = np.array(tfidf_model.transform(df[\"query\"]).sum(1)).squeeze()\n",
    "        df['%s_TLen' % prefix] = t_vec_len\n",
    "        df['%s_QTLenRatio'%prefix] = list(map(lambda a, b: try_divide(a, b), q_vec_len, t_vec_len))\n",
    "        df['%s_TQLenRatio'%prefix] = list(map(lambda a, b: try_divide(b, a), q_vec_len, t_vec_len))\n",
    "        df['%s_QTDiff'%prefix] = list(map(lambda a, b: abs(a-b), q_vec_len, t_vec_len))\n",
    "        df['%s_QTMax'%prefix] = list(map(lambda a, b: max(a, b), q_vec_len, t_vec_len))\n",
    "        df['%s_QTMin'%prefix] = list(map(lambda a, b: min(a, b), q_vec_len, t_vec_len))\n",
    "        df['%s_QTAvg'%prefix] = list(map(lambda a, b: (a+b)/2, q_vec_len, t_vec_len))\n",
    "        df['%s_QTMulti'%prefix] = list(map(lambda a, b: a*b, q_vec_len, t_vec_len))\n",
    "    del t_list, t_vec_len, q_vec_len\n",
    "    gc.collect()\n",
    "    return df\n",
    "    \n",
    "def process_diff_set_tfidf_len(df, save_dir, prefix, feature_name):\n",
    "    savefile = os.path.join(save_dir, '%s_%s' % (prefix, feature_name))\n",
    "    df = run_diff_set_tfidf_len(df, ngram_utils.unigrams, \"{}{}\".format(prefix, \"Unigram\"), \"{}{}\".format(savefile,\"Unigram\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "E07D336F0F2B468C84098E12D421F7B8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_diffSet_tfidf_len:\n",
    "    ExtractFeature(train_file, feature_save_dir, train_feature_prefix, feature_name, \n",
    "        process_func=process_diff_set_tfidf_len, names=ORI_TRAIN_NAMES, process_chunkly=False, \n",
    "        chunk_size=CHUNK_SIZE, drop_first_cols=['label', 'query_title_id'], \n",
    "        drop_last_cols=['query_id', 'query', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2570D7A6D15A40A4BDC5AA14272C062F",
    "mdEditEnable": false
   },
   "source": [
    "### 点击率特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "11F0EF4C03A04CA982BE108D9CB0B36B",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regen_ctr_rate = False\n",
    "feature_name = \"ctrLocal\"\n",
    "stat = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "39BF45B506314BF9A8D04616B20D221A",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ExtractLocalCTR(df, save_dir, prefix, feature_name):\n",
    "    global stat\n",
    "    grouped = df.groupby(\"query_id\", as_index=False)\n",
    "    count = grouped.size().values\n",
    "    if not stat:\n",
    "        label = grouped[\"label\"].sum()[\"label\"].values.astype(\"int32\")\n",
    "        for _ in range(len(count)):\n",
    "            item_len = count[_]\n",
    "            if item_len not in stat:\n",
    "                stat[item_len] = [1, label[_]]\n",
    "            else:\n",
    "                stat[item_len][0] += 1\n",
    "                stat[item_len][1] += label[_]\n",
    "        del label\n",
    "    res = [[ stat[_][1] / (_ * stat[_][0])]*_ for _ in count]\n",
    "    result = []\n",
    "    for _ in res:\n",
    "        result.extend(_)\n",
    "    df[\"{}_localCTR\".format(prefix)] = result\n",
    "    del res, grouped\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "33E353FEFC7D4577A377992D5FE489C5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regen_ctr_rate:\n",
    "    ExtractFeature(train_file, feature_save_dir, train_feature_prefix, feature_name, \n",
    "    ExtractLocalCTR, names=ORI_TRAIN_NAMES,process_chunkly=False, \n",
    "    drop_first_cols=['query_title_id', 'query', 'title'], drop_last_cols=['query_id', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "6360746B670045F4BB764785F47B713C",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not OFFLINE and regen_ctr_rate:\n",
    "    ExtractFeature(test_file, feature_save_dir, test_feature_prefix, feature_name, \n",
    "    ExtractLocalCRT, names=ORI_TEST_NAMES,process_chunkly=False, \n",
    "    drop_first_cols=['query_title_id', 'query', 'title'], drop_last_cols=['query_id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
