{"cells":[{"cell_type":"code","metadata":{"id":"D465F35808444728A922E47BAEE20537","collapsed":false,"scrolled":false},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport time\nimport random\nimport gc\nimport sys\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom utils import ReadCSV, ProcessChunk, Timer, RandomSample","execution_count":4},{"metadata":{"id":"A95285FB6CF640F7A8DB50B9473AF455","mdEditEnable":false},"cell_type":"markdown","source":"## 参数设置"},{"cell_type":"code","metadata":{"id":"75AAEDDB844E4F188DB56151C0126314","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"total 18592256\r\ndrwxr-xr-x 5 kesci root        4096 Jun  2 13:12 .\r\ndrwxrwxrwx 7 root  root        4096 Jun  2 12:46 ..\r\n-rwxr-xr-x 1 kesci users    7842088 May 25 15:15 kesci_submit\r\n-rw-r--r-- 1 kesci users 9027388177 Jun  2 12:56 labeled_content\r\ndrwx------ 2 root  root       16384 May 26 04:29 lost+found\r\ndrwxr-xr-x 2 kesci users       4096 Jun  1 12:52 __pycache__\r\n-rw-r--r-- 1 root  root          38 May 26 04:30 .sidecarDownloadOnce\r\ndrwxr-xr-x 2 kesci users       4096 Jun  2 07:41 stats\r\n-rw-r--r-- 1 kesci users  162605370 May 29 09:14 submit1.csv\r\n-rw-r--r-- 1 kesci users   64918110 May 31 12:41 submit_baseline_0.0.csv\r\n-rw-r--r-- 1 kesci users   64918110 May 31 12:43 submit_baseline_0.5.csv\r\n-rw-r--r-- 1 kesci users   64918110 May 31 12:38 submit_baseline_1.0.csv\r\n-rw-r--r-- 1 kesci users  162604756 May 28 17:25 submit.csv\r\n-rw-r--r-- 1 kesci users  455810675 May 28 13:36 test_processed.csv\r\n-rw-r--r-- 1 kesci users 8126569615 Jun  2 13:12 train.txt\r\n-rw-r--r-- 1 kesci root        2425 Jun  1 12:51 utils.py\r\n-rw-r--r-- 1 kesci users  900818562 Jun  2 13:12 valid.txt\r\n","name":"stdout"}],"source":"# 是否为线下调试\ndebug = False\n# ! mkdir stats\n! ls -al","execution_count":1},{"cell_type":"code","metadata":{"id":"525F9E74396E458CB3C9885399AA8B89","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"total 9.0G\r\n-rw-r--r-- 1 kesci 1000 426M May 22 04:05 test.csv\r\n-rw-r--r-- 1 kesci 1000 8.6G May 16 20:15 train.csv\r\n","name":"stdout"}],"source":"# 数据集参数\nif debug:\n    !ls -lh ../data\n    data_dir = \"../data/\"\n    train_data_file = data_dir + \"train_data.csv\"\n    test_data_file = data_dir + \"test_data.csv\"\nelse:\n    !ls -lh /home/kesci/input/bytedance/first-round/\n    data_dir = \"/home/kesci/input/bytedance/first-round/\"\n    train_data_file = data_dir + \"train.csv\"\n    test_data_file = data_dir + \"test.csv\"\n# csv的header\nori_train_names = [\"query_id\", \"query\", \"query_title_id\", \"title\", \"label\"]\nori_test_names = [\"query_id\", \"query\", \"query_title_id\", \"title\"]","execution_count":3},{"metadata":{"id":"FE6B332489134F518C94F4D44A87B69C","mdEditEnable":false},"cell_type":"markdown","source":"## 辅助打印函数"},{"cell_type":"code","metadata":{"id":"C476EDB46DEE4FB1941662A49BF1626E","collapsed":false,"scrolled":false},"outputs":[],"source":"def Plot(plot_param, \n         plot_func,\n         xlabel,\n         ylabel,\n         savefile,\n         figsize=(8, 4), \n         vertical=False):\n    plt.figure(figsize=figsize)\n    plot_func(**plot_param)\n    plt.xlabel(xlabel, fontsize=12)\n    plt.ylabel(ylabel, fontsize=12)\n    if vertical:\n        plt.xticks(rotation='vertical')\n    plt.savefig(savefile)\n    plt.show()","execution_count":4},{"cell_type":"code","metadata":{"id":"40D18812D9494A73AF691113965FB88E","collapsed":false,"scrolled":false},"outputs":[],"source":"def PlotBar(dfx, dfy, \n            xlabel, \n            ylabel, \n            savefile, \n            figsize=(8, 4), \n            vertical=False):    \n    Plot({\"x\":dfx, \"y\":dfy, \"alpha\":0.8}, \n         sns.barplot,\n         xlabel, \n         ylabel, \n         savefile,\n         figsize,\n         vertical\n        )","execution_count":5},{"cell_type":"code","metadata":{"id":"4854EDDBA6BA495F8F67FA4F80549940","collapsed":false,"scrolled":false},"outputs":[],"source":"def PlotBox(df, \n            x, y, \n            xlabel, \n            ylabel, \n            savefile, \n            figsize=(12, 6),\n            vertical=False):\n    Plot({\"x\":x, \"y\":y, \"data\":df}, \n         sns.boxplot,\n         xlabel, \n         ylabel, \n         savefile,\n         figsize,\n         vertical\n        )","execution_count":6},{"cell_type":"code","metadata":{"id":"B007261547114D099829F925EF8C4E08","collapsed":false,"scrolled":false},"outputs":[],"source":"def PlotPoint(x, y, \n              xlabel, \n              ylabel, \n              savefile, \n              figsize=(12, 8), \n              vertical=False):\n    Plot({\"x\":x, \"y\":y, \"alpha\":0.8}, \n         sns.pointplot,\n         xlabel, \n         ylabel, \n         savefile,\n         figsize,\n         vertical\n        )","execution_count":7},{"metadata":{"id":"6196273BF67D48D18EE4E87DA68BA93C","mdEditEnable":false},"cell_type":"markdown","source":"## 分析函数"},{"cell_type":"code","metadata":{"id":"E6DAEEE566C845589F9FC160AD9A9838","collapsed":false,"scrolled":false},"outputs":[],"source":"# 看label的分布\ndef AnalysisLabel(df):\n    print(\"--->Label Ananlysis\")\n    count_label = df[\"label\"].value_counts()\n    PlotBar(count_label.index, \n            count_label.values, \n            xlabel=\"Clicked\",\n            ylabel=\"Number of occurrences\",\n            savefile= stat_save_dir+\"label_dis.png\"\n           )\n    print(count_label / count_label.sum())\n    del count_label\n    gc.collect()","execution_count":9},{"cell_type":"code","metadata":{"id":"B6B79AAEE3DF4C8E87237F7D0B5FD7DA","collapsed":false,"scrolled":false},"outputs":[],"source":"# 看feature的单词数目分布\ndef AnalysisFeatureWordNum(df, name=\"query\", label_rel=True):\n    print(\"--->Word num in feature \" + name)\n    # print(df[name][:10])\n    tmp = df[name].apply(\n        lambda x: len(str(x).split(\" \"))\n    )\n    # print(tmp[:10])\n    tmp_count = tmp.value_counts()\n    PlotBar(tmp_count.index,  # feature中单词的数目\n            tmp_count.values,  # 固定数目feature出现的次数\n            xlabel=\"Number of words in \" + name,\n            ylabel=\"Number of occurrences\",\n            savefile=stat_save_dir+\"word_num_in_\"+name+\".png\",\n            vertical=True\n           )\n    if label_rel:\n        num_df = pd.DataFrame({\n            \"feature_word_num_log1p\": np.log1p(tmp),\n            \"label\": df[\"label\"]\n        })\n        PlotBox(num_df, x=\"label\", y=\"feature_word_num_log1p\", \n                xlabel=\"clicked\", ylabel=name+\" word num log1p\",\n                savefile=stat_save_dir+\"word_num_box_in_\"+name+\".png\"\n               )\n        del num_df\n    \n    del tmp\n    gc.collect()","execution_count":10},{"cell_type":"code","metadata":{"id":"92EDCCF0A0FD420D89D454C6B41D47BE","collapsed":false,"scrolled":false},"outputs":[],"source":"# 看组合feature的单词数目分布\ndef AnalysisConcatWordNum(df, label_rel=True):\n    print(\"--->Word num in concat feature\")\n    tmp = df.apply(\n        lambda x : len(str(x[\"query\"]).split(\" \")) + len(str(x[\"title\"]).split(\" \")),\n        axis = 1\n    )\n    # print(tmp[:10])\n    feature_words_count = tmp.value_counts()\n    PlotBar(feature_words_count.index, \n            feature_words_count.values, \n            xlabel=\"Number of words in concat feature\",\n            ylabel=\"Number of occurrences\",\n            savefile=stat_save_dir+\"word_num_in_concat.png\",\n            vertical=True\n           )\n    \n    if label_rel:\n        tmp_df = pd.DataFrame({\n            \"concat_word_num_log1p\": np.log1p(tmp),\n            \"label\": df[\"label\"]\n        })\n        PlotBox(tmp_df, x=\"label\", y=\"concat_word_num_log1p\", \n                xlabel=\"clicked\", ylabel=\"concat word num log1p\",\n                savefile=stat_save_dir+\"word_num_box_in_concat.png\"\n               )\n        del tmp_df\n        \n    # 统计字符级别的长度\n#     all_feature_df[\"concat_chars_count\"] = all_feature_df[\"features\"].apply(\n#         lambda x: len(str(x))\n#     )\n#     feature_chars_count = all_feature_df[\"concat_chars_count\"].value_counts()\n#     PlotBar(feature_chars_count.index, \n#             feature_chars_count.values, \n#             xlabel=\"Number of characters in concat\",\n#             ylabel=\"Number of occurrences\",\n#             vertical=True\n#            )\n    \n    del tmp\n    gc.collect()","execution_count":11},{"cell_type":"code","metadata":{"id":"EC832B5B9FF745C38C10A93C33ADB7CF","collapsed":false,"scrolled":false},"outputs":[],"source":"# 分析两个特征UniGram关系\ndef AnalysisGrams(df):\n    print(\"--->Common gram stat\")\n    df[\"common_unigram_count\"] = df.apply(\n        lambda x: len(set(str(x[\"query\"]).split()).intersection(\n            set(str(x[\"title\"]).split())\n        )),\n        axis=1\n    )\n    df[\"common_unigram_ratio\"] = df.apply(\n        lambda x: float(x[\"common_unigram_count\"]) / max(\n                len(\n                    set(str(x[\"query\"]).split()).union(\n                        set(str(x[\"title\"]).split())\n                    )\n                ), \n                1\n            )\n        ,\n        axis=1\n    )    \n    cnt_srs = df['common_unigram_count'].value_counts()\n    PlotBar(cnt_srs.index, \n            cnt_srs.values, \n            xlabel='Common unigram count',\n            ylabel=\"Number of occurrences\",\n            savefile=stat_save_dir+\"common_unigram_count.png\",\n            vertical=True\n           )\n    PlotBox(df, x=\"label\", y=\"common_unigram_count\", \n            xlabel=\"clicked\", ylabel=\"common unigram count\",\n            savefile=stat_save_dir+\"common_unigram_count_box.png\"\n           )\n    PlotBox(df, x=\"label\", y=\"common_unigram_ratio\", \n            xlabel=\"clicked\", ylabel=\"common unigram radio\",\n           savefile=stat_save_dir+\"common_unigram_radio_box.png\")\n    # print(df[:3])\n    df.drop(['common_unigram_count', 'common_unigram_ratio'], axis=1)\n    gc.collect()\n    ","execution_count":12},{"metadata":{"id":"9533442B115148C581EB741304E5884A","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 分析两个feature的交集关系\ndef AnalysisIntersect(df, q_dict):\n    # 这里的频率是一个query会对应几个title\n    def q1Freq(row):\n        return len(q_dict[row['query']])\n    def q2Freq(row):\n        return len(q_dict[row['title']]) \n    def q1q2FreqIntersect(row):\n        return len(\n            set(q_dict[row['query']]\n        ).intersection(\n                set(q_dict[row['title']])\n        ))\n    \n    freq_df = pd.DataFrame(\n          {\n            \"query_freq\":df.apply(\n                q1Freq, axis=1, raw=True\n            ),\n            \"title_freq\":df.apply(\n                q2Freq, axis=1, raw=True\n            ),\n            \"query_title_intersect\":df.apply(\n                q1q2FreqIntersect, axis=1, raw=True\n            ),\n            \"label\": df[\"label\"]\n        }\n    )\n    \n    # 看交集出现的次数\n    cnt_srs = freq_df['query_title_intersect'].value_counts()\n    PlotBar(cnt_srs.index, \n            np.log1p(cnt_srs.values), \n            xlabel='Query-Title neighbor intersection count',\n            ylabel=\"Log of Number of Occurrences\",\n            vertical=True,\n            savefile=stat_save_dir+\"query_title_intersect_count.png\"\n           )\n    \n    # 看交集出现次数跟label的关系\n    grouped_df = freq_df.groupby('query_title_intersect')['label'].aggregate(np.mean).reset_index()\n    PlotPoint(\n        grouped_df[\"query_title_intersect\"].values,\n        grouped_df[\"label\"].values,\n        ylabel=\"Mean clicked\",\n        xlabel=\"Q1-Q2 neighbor intersection count\",\n        vertical=True,\n        savefile=stat_save_dir+\"query_title_intersect_impact_label.png\"\n    )\n    \n    # 看每个query的title次数\n    cnt_srs = freq_df['query_freq'].value_counts()\n    PlotBar(cnt_srs.index, \n            cnt_srs.values, \n            xlabel='Query Frequency',\n            ylabel=\"Number of Occurrences\",\n            vertical=True,\n            savefile=stat_save_dir+\"query_frequency_count.png\"\n           )\n    \n    # 看每个title的query次数\n    cnt_srs = freq_df['title_freq'].value_counts()\n    PlotBar(cnt_srs.index, \n            cnt_srs.values, \n            xlabel='Title Frequency',\n            ylabel=\"Number of Occurrences\",\n            vertical=True,\n            savefile=stat_save_dir+\"title_frequency_count.png\"\n           )\n    \n    # 看query出现频率对于label的影响\n    grouped_df = freq_df.groupby('query_freq')['label'].aggregate(np.mean).reset_index()\n    PlotPoint(\n        grouped_df[\"query_freq\"].values,\n        grouped_df[\"label\"].values,\n        ylabel=\"Mean clicked\",\n        xlabel=\"Q1 Frequency\",\n        vertical=True,\n        savefile=stat_save_dir+\"query_impact_label.png\"\n    )\n    \n    # 数据透视表: https://blog.csdn.net/qq_36495431/article/details/81123240\n    # 看query频率、title频率与label的关系\n    pvt_df = freq_df.pivot_table(index=\"query_freq\", \n                                 columns=\"title_freq\", \n                                 values=\"label\")\n    plt.figure(figsize=(12,12))\n    sns.heatmap(pvt_df)\n    plt.title(\"Mean clicked value distribution across query and title frequency\")\n    plt.savefig(stat_save_dir+\"query_title_impact_label.png\")\n    plt.show()\n    \n    # 三者关系\n    cols_to_use = ['query_title_intersect', 'query_freq', 'title_freq']\n    tmp_df = freq_df[cols_to_use]\n    corrmat = tmp_df.corr(method='spearman')\n    f, ax = plt.subplots(figsize=(8, 8))\n    sns.heatmap(corrmat, vmax=1., square=True)\n    plt.title(\"Leaky variables correlation map\", fontsize=15)\n    plt.savefig(stat_save_dir+\"correlation.png\")\n    plt.show()\n    \n    del all_df, q_dict, freq_df, grouped_df, pvt_df, tmp_df\n    gc.collect()","execution_count":12},{"metadata":{"id":"514F810A6F6F45E0B21C10F0AC348711","mdEditEnable":false},"cell_type":"markdown","source":"## 分析"},{"cell_type":"code","metadata":{"id":"19E9853F779343A6ABB00FC4706BD935","collapsed":false,"scrolled":false},"outputs":[],"source":"stat_save_dir = \"./stats/\"","execution_count":9},{"metadata":{"id":"987311FFD6BE4D1E865F6E4906500157","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Random sample...\nFinished sample.\n","name":"stdout"}],"source":"test_df = ReadCSV(test_data_file, names=ori_test_names, iterator=False)\ndf = RandomSample(train_data_file, 0.2, names=ori_train_names, chunk_size=10000000)","execution_count":10},{"metadata":{"id":"B8C4D20F3ED64FB4807F2F0CFFDA275D","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Shape concated train and test: (25000000, 2)\n----->Started 'Create dict' block...\n----->Finished 'Create dict' block, time used: 3722.74s.\n","name":"stdout"}],"source":"all_df = pd.concat([df[[\"query\", \"title\"]], \n                        test_df[[\"query\", \"title\"]]],\n                      axis=0).reset_index(drop=\"index\")\nprint(\"Shape concated train and test:\", all_df.shape)\n\nwith Timer(\"Create dict\"):\n    from collections import defaultdict                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n    q_dict = defaultdict(set)\n    for _ in range(all_df.shape[0]):\n        # 存储的是一个query对应哪几个title和\n        # 一个title对应哪几个query\n        q_dict[all_df[\"query\"][_]].add(all_df[\"title\"][_])\n        q_dict[all_df[\"title\"][_]].add(all_df[\"query\"][_])","execution_count":11},{"metadata":{"id":"7DC6252905D842F7B957588387BAEB45","collapsed":false,"scrolled":true},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 576x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/7DC6252905D842F7B957588387BAEB45/psgnzb4dhc.png\">"}},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 864x576 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/7DC6252905D842F7B957588387BAEB45/psgnzdjvya.png\">"}},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 576x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/7DC6252905D842F7B957588387BAEB45/psgnzgtoh7.png\">"}},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 576x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/7DC6252905D842F7B957588387BAEB45/psgnznhbkp.png\">"}},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 864x576 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/7DC6252905D842F7B957588387BAEB45/psgnzqn644.png\">"}},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 864x864 with 2 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/7DC6252905D842F7B957588387BAEB45/psgnzvregx.png\">"}},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 576x576 with 2 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/7DC6252905D842F7B957588387BAEB45/psgo1h4o9d.png\">"}},{"output_type":"error","ename":"UnboundLocalError","evalue":"local variable 'all_df' referenced before assignment","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-b334fe9c9085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAnalysisIntersect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-196fd681f4bc>\u001b[0m in \u001b[0;36mAnalysisIntersect\u001b[0;34m(df, q_dict)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0;32mdel\u001b[0m \u001b[0mall_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrouped_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvt_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'all_df' referenced before assignment"]}],"source":"AnalysisIntersect(df, q_dict)","execution_count":13},{"cell_type":"code","metadata":{"id":"5B1CFCE33E514B0094C6A570395CF115","collapsed":false,"scrolled":false},"outputs":[],"source":"# 原始数据集上的分析\ndef AnalysisDataset(trainfile, testfile,\n                     train_names=ori_train_names, \n                     test_names=ori_test_names,\n                     chunk_size=5000000):\n#     chunks = []\n#     ap = lambda x: chunks.append(x)\n#     ProcessChunk(trainfile, ap, train_names, chunk_size)\n#     df = pd.concat(chunks, ignore_index=True)\n    test_df = ReadCSV(testfile, names=test_names, iterator=False)\n    df = RandomSample(trainfile, 0.2, names=ori_train_names, chunk_size=chunk_size)\n    \n#     # 可以直接跑出来\n    # AnalysisLabel(df)\n    # AnalysisFeatureWordNum(df, name=\"query\")\n    # AnalysisFeatureWordNum(df, name=\"title\")\n    \n    # concatLen, label = [], []\n    # def ap(x):\n    #     tmp = genConcatLen(x)\n    #     concatLen.append0(tmp[0])\n    #     label.append(tmp[1])\n    # ProcessChunk(trainfile, ap, train_names, chunk_size)\n    # concatLen = pd.concat(concatLen)\n    # label = pd.concat(label)\n    # AnalysisConcatWordNum(concatLen, label)\n    \n    # AnalysisConcatWordNum(df)\n    # AnalysisGrams(df)\n    \n    # 混合关联\n    AnalysisIntersect(df, test_df)\n\nif debug:\n    chunk_size = 1000\nelse:\n    chunk_size = 10000000\n    \n# AnalysisDataset(train_data_file, test_data_file, chunk_size=chunk_size)","execution_count":15}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}