{"cells":[{"metadata":{"id":"E4151556EDD340429BBB8A745AFE7741","mdEditEnable":false},"cell_type":"markdown","source":"# 模型训练"},{"metadata":{"id":"1602551E42724678845E9497C55C523A","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"total 21G\r\ndrwxr-xr-x 2 kesci root  4.0K Jul 31 14:33 .\r\ndrwxr-xr-x 6 kesci root  4.0K Jul 28 14:30 ..\r\n-rw-r--r-- 1 kesci root   19K Jul 29 13:56 DeepFM.py\r\n-rw-r--r-- 1 kesci users 249M Jul 22 03:01 tfidf_word_ngram-13-100w.bin\r\n-rw-r--r-- 1 kesci users 6.2G Jul 23 17:48 tfidf_word_ngram-13.bin\r\n-rw-r--r-- 1 kesci users 929M Jul 31 12:26 tfidf_word_ngram-13_debug_allUnique.bin\r\n-rw-r--r-- 1 kesci users 702M Jul 31 12:36 tfidf_word_ngram-13_debug_justTrain.bin\r\n-rw-r--r-- 1 kesci users 929M Jul 31 09:26 tfidf_word_ngram-13_debug_noUnique.bin\r\n-rw-r--r-- 1 kesci users 1.1G Jul 31 12:53 tfidf_word_ngram-13_debug_queryConcatTitle.bin\r\n-rw-r--r-- 1 kesci users 929M Jul 31 09:30 tfidf_word_ngram-13_debug_queryUnique.bin\r\n-rw-r--r-- 1 kesci users 7.6G Jul 31 16:11 tfidf_word_ngram-13_online_allUnique.bin\r\n-rw-r--r-- 1 kesci users  11M Jul 31 13:29 word2vec_debug_allUnique.kv\r\n-rw-r--r-- 1 kesci users  70M Jul 31 13:29 word2vec_debug_allUnique.kv.vectors.npy\r\n-rw-r--r-- 1 kesci users  11M Jul 31 13:26 word2vec_debug_allUnique.model\r\n-rw-r--r-- 1 kesci users  70M Jul 31 13:26 word2vec_debug_allUnique.model.trainables.syn1neg.npy\r\n-rw-r--r-- 1 kesci users  70M Jul 31 13:26 word2vec_debug_allUnique.model.wv.vectors.npy\r\n-rw-r--r-- 1 kesci users 9.6M Jul 31 13:58 word2vec_debug_concatVertialUnique.kv\r\n-rw-r--r-- 1 kesci users  66M Jul 31 13:58 word2vec_debug_concatVertialUnique.kv.vectors.npy\r\n-rw-r--r-- 1 kesci users  11M Jul 31 13:58 word2vec_debug_concatVertialUnique.model\r\n-rw-r--r-- 1 kesci users  66M Jul 31 13:58 word2vec_debug_concatVertialUnique.model.trainables.syn1neg.npy\r\n-rw-r--r-- 1 kesci users  66M Jul 31 13:58 word2vec_debug_concatVertialUnique.model.wv.vectors.npy\r\n-rw-r--r-- 1 kesci users  35M Jul 21 07:09 word2vec.kv\r\n-rw-r--r-- 1 kesci users 465M Jul 21 07:09 word2vec.kv.vectors.npy\r\n-rw-r--r-- 1 kesci users  37M Jul 24 03:56 word2vec.model\r\n-rw-r--r-- 1 kesci users 465M Jul 24 03:56 word2vec.model.trainables.syn1neg.npy\r\n-rw-r--r-- 1 kesci users 465M Jul 24 03:56 word2vec.model.wv.vectors.npy\r\n","name":"stdout"}],"source":"import pickle, sys, gc, time, os\nimport pandas as pd\nimport numpy as np\nimport csv\nsys.path.append(\"./common\")\nmodel_save_dir = \"/home/kesci/work/common/util_models/\"\nfrom utils import ReadCSV, Timer, ExtractFeature, ORI_TRAIN_NAMES, ORI_TEST_NAMES, \\\nProcessChunk, AnalysisCSV, ReadCSV1\nOFFLINE = False\ntrain_name = ORI_TRAIN_NAMES\nif OFFLINE:\n    train_file = \"/home/kesci/work/inputs/train_flast_100w.csv\"\n    test_file = \"/home/kesci/work/inputs/train_flast_40w.csv\"\n    test_name = ORI_TRAIN_NAMES\nelse:\n    train_file = \"/home/kesci/work/inputs/train_first_5000w.csv\"\n    test_file = \"/home/kesci/work/inputs/test_final_part1.csv\"\n    test_name = ORI_TEST_NAMES\n! ls -alh /home/kesci/work/common/util_models/","execution_count":2},{"metadata":{"cell_type":"code","id":"8E4453F4DEA64B5D8B98370A92A46F4F","mdEditEnable":false},"cell_type":"markdown","source":"## 训练TFIDF"},{"metadata":{"id":"02FC27ED524647EEBFBFF24B8C2C6683","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"/home/kesci/work/common/util_models/tfidf_word_ngram-13_debug_queryConcatTitle.bin\n","name":"stdout"}],"source":"from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.decomposition import TruncatedSVD\nngram_range, ngram_level = (1, 3), \"word\"\nsuffix = \"allUnique\"\nif OFFLINE:\n    mode = \"debug\"\nelse:\n    mode = \"online\"\ntfidf_save_file = \"{}tfidf_{}_ngram-{}{}_{}_{}.bin\".format(model_save_dir, ngram_level, \n    ngram_range[0], ngram_range[1], mode, suffix)\nregen_tfidf_model = True\nprint(tfidf_save_file)","execution_count":28},{"metadata":{"id":"8791ED9A55D745EBBD3032CAD1C88715","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def train_tfidf(save_file):\n    word_tfidf = TfidfVectorizer(norm=\"l2\",  # 'l2 norm' can cal similarty\n                            strip_accents=\"unicode\",\n                            analyzer=ngram_level,\n                            ngram_range=ngram_range,\n                            use_idf=True,\n                            smooth_idf=True,\n                            sublinear_tf=True, min_df=2, max_df=0.9)\n    with Timer(\"prepare corpus\"):\n        df_train = ReadCSV(train_file, names=train_name, iterator=False)\n        df_test = ReadCSV(test_file, names=test_name,  iterator=False)\n        df_data = pd.concat([df_train[['query', 'title']], df_test[['query', 'title']]], axis=0)\n        del df_train, df_test; gc.collect()\n        corpus =  pd.Series(\n            df_data['query'].unique().tolist() +\n            df_data['title'].unique().tolist()\n        ).astype(str)\n        print(corpus.shape)\n    with Timer(\"train tfidf\"):\n        word_tfidf.fit(corpus)\n    with Timer(\"save tfidf\"):\n        with open(save_file, \"wb\") as f:\n            pickle.dump(word_tfidf, f)\n            print(\"Model dumped to {}\".format(save_file))\n        del corpus\n        gc.collect()\n    return None","execution_count":29},{"metadata":{"id":"BEF6DEBC24CD401189BDDA18255C8B74","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"----->started 'prepare corpus' block...\n----->finished 'prepare corpus' block, time used:126.31s.\n----->started 'train tfidf' block...\n----->finished 'train tfidf' block, time used:175.6s.\n----->started 'save tfidf' block...\nModel dumped to /home/kesci/work/common/util_models/tfidf_word_ngram-13_debug_queryConcatTitle.bin\n----->finished 'save tfidf' block, time used:66.29s.\n","name":"stdout"}],"source":"# 提取训练数据特征\nif regen_tfidf_model:\n    train_tfidf(tfidf_save_file)","execution_count":30},{"metadata":{"id":"CF670F15F4154BF187A3C1AC441791FB","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# AnalysisCSV(train_file, print_rows=10, names=ORI_TRAIN_NAMES)","execution_count":9},{"metadata":{"id":"401827997544459A82EF4942AC6B3B55","mdEditEnable":false},"cell_type":"markdown","source":"## 训练word2vec"},{"metadata":{"id":"148D84B4A88B4FFC8AC94CD1FA5B4B8A","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"from gensim.models import Word2Vec\nfrom gensim.models.word2vec import LineSentence\nfrom gensim.models.callbacks import CallbackAny2Vec","execution_count":3},{"metadata":{"id":"2A33E935416F465C84CB2DEB06798D59","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"regen_word2vec = True\nsuffix = \"concatHorizontal\"\nif OFFLINE:\n    mode = \"debug\"\n    word2vec_train_sentences = \"/home/kesci/work/inputs/word2vec_sentences_flast_100w.txt\"\nelse:\n    mode = \"online\"\n    word2vec_train_sentences = \"/home/kesci/work/inputs/word2vec_sentences_flast_5000w.txt\"\nword2vec_save_name = \"word2vec_{}_{}.model\".format(mode, suffix)\nword2vec_save_file = os.path.join(model_save_dir, word2vec_save_name)\nword2vec_kv_save_name = \"word2vec_{}_{}.kv\".format(mode, suffix)\nword2vec_kv_save_file = os.path.join(model_save_dir, word2vec_kv_save_name)","execution_count":4},{"metadata":{"id":"6A7FB5977F4546A987C865805507FBB1","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"----->started 'prepare corpus' block...\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3248: DtypeWarning: Columns (1,3) have mixed types. Specify dtype option on import or set low_memory=False.\n  if (await self.run_code(code, result,  async_=asy)):\n","name":"stderr"},{"output_type":"stream","text":"----->finished 'prepare corpus' block, time used:113.82s.\n----->started 'save sentences' block...\nProcessed 10000000 lines.\nProcessed 20000000 lines.\nProcessed 30000000 lines.\nProcessed 40000000 lines.\nProcessed 50000000 lines.\nProcessed 60000000 lines.\nProcessed 70000000 lines.\n----->finished 'save sentences' block, time used:6054.99s.\n","name":"stdout"}],"source":"def PrepareWord2vecSamples(save_file):\n    with Timer(\"prepare corpus\"):\n        df_train = pd.read_csv(train_file, names=train_name)\n        df_test = pd.read_csv(test_file, names=test_name)\n        corpus = pd.concat([df_train[['query', 'title']], df_test[['query', 'title']]], axis=0)\n        del df_train, df_test\n        gc.collect()\n    with Timer(\"save sentences\"):\n        with open(save_file, 'w') as f:\n            line_count = 0\n            for i, line in corpus.iterrows():\n                f.write(\"{} {}\\n\".format(line[\"query\"], line[\"title\"]))\n                line_count += 1\n                if line_count % 10000000 == 0: \n                    print(f'Processed {line_count} lines.')\n            del corpus\n            gc.collect()\n# PrepareWord2vecSamples(word2vec_train_sentences)","execution_count":5},{"metadata":{"id":"6DBCAC48A14A4833807190511F845215","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"844 10482 6804 8411\r\n844 6026 15 53 100\r\n51685 402\r\n1913 1278 15 2215 1039\r\n4311 456 15 321608 95\r\n53058 31 50989 3028\r\n236 127 1364 55882 2824\r\n236 277 3705 2848 10035\r\n236 160931 10592\r\n236 68629 5975 100 29816 87051\r\n","name":"stdout"}],"source":"!head /home/kesci/work/inputs/word2vec_sentences_flast_100w.txt","execution_count":7},{"metadata":{"id":"B3FA87E72D5B45C4B46AD90DF7EC072C","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class EpochSaver(CallbackAny2Vec):\n    '''用于保存模型, 打印损失函数等等'''\n    def __init__(self, savedir, save_name=\"word2vec.model\"):\n        os.makedirs(savedir, exist_ok=True)\n        self.save_path = os.path.join(savedir, save_name)\n        self.epoch = 0\n        self.pre_loss = 0\n        self.best_loss = 999999999.9\n        self.since = time.time()\n\n    def on_epoch_end(self, model):\n        self.epoch += 1\n        cum_loss = model.get_latest_training_loss() # 返回的是从第一个epoch累计的\n        epoch_loss = cum_loss - self.pre_loss\n        time_taken = time.time() - self.since\n        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" % \n                    (self.epoch, epoch_loss, time_taken//60, time_taken%60))\n        if self.best_loss > epoch_loss and epoch_loss > 0:\n            self.best_loss = epoch_loss\n            print(\"Better model. Best loss: %.2f\" % self.best_loss)\n            model.save(self.save_path)\n            print(\"Model %s save done!\" % self.save_path)\n        self.pre_loss = cum_loss\n        self.since = time.time()","execution_count":8},{"metadata":{"id":"FBAA2759BE184CC8B7CFA68DE2BE5AEC","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"if regen_word2vec:\n    # 1, 构建模型(不训练)\n    model_word2vec = Word2Vec(min_count=2, \n                              window=10, \n                              size=100,\n                              workers=10)","execution_count":9},{"metadata":{"id":"9F7E78168C90423FACC65FC69CF09B85","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n","name":"stderr"},{"output_type":"stream","text":"Time to build vocab: 7min 58s\n","name":"stdout"}],"source":"if regen_word2vec:\n    # 2, 遍历一遍语料库\n    since = time.time()\n    model_word2vec.build_vocab(\n        LineSentence(word2vec_train_sentences)\n    )\n    time_elapsed = time.time() - since\n    print('Time to build vocab: {:.0f}min {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))","execution_count":10},{"metadata":{"id":"E18741A5CAC4491597F1205C7C08F1A0","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Epoch 1, loss: 69566760.00, time: 18min 33s\nBetter model. Best loss: 69566760.00\nModel /home/kesci/work/common/util_models/word2vec_online_concatHorizontal.model save done!\nEpoch 2, loss: 23542272.00, time: 18min 24s\nBetter model. Best loss: 23542272.00\nModel /home/kesci/work/common/util_models/word2vec_online_concatHorizontal.model save done!\nEpoch 3, loss: 24322496.00, time: 18min 20s\nEpoch 4, loss: 16786200.00, time: 18min 19s\nBetter model. Best loss: 16786200.00\nModel /home/kesci/work/common/util_models/word2vec_online_concatHorizontal.model save done!\nEpoch 5, loss: 0.00, time: 18min 17s\nEpoch 6, loss: 0.00, time: 18min 18s\nEpoch 7, loss: 0.00, time: 18min 20s\nEpoch 8, loss: 0.00, time: 18min 20s\nEpoch 9, loss: 0.00, time: 18min 19s\nEpoch 10, loss: 0.00, time: 18min 17s\nEpoch 11, loss: 0.00, time: 18min 19s\nEpoch 12, loss: 0.00, time: 18min 20s\nEpoch 13, loss: 0.00, time: 18min 20s\nEpoch 14, loss: 0.00, time: 18min 20s\nEpoch 15, loss: 0.00, time: 18min 20s\nEpoch 16, loss: 0.00, time: 18min 18s\nEpoch 17, loss: 0.00, time: 18min 18s\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-70dd2299297f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_word2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         callbacks=[EpochSaver(model_save_dir, word2vec_save_name)])\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msince\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time to train: {:.0f}min {:.0f}s'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_elapsed\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m             trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    256\u001b[0m                 \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                 queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mtrained_word_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrained_word_count_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mraw_word_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mraw_word_count_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    225\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    226\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             report_delay=report_delay)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":"if regen_word2vec:\n    # 3, 训练\n    since = time.time()\n    model_word2vec.train(\n        LineSentence(word2vec_train_sentences), \n        total_examples=model_word2vec.corpus_count, \n        epochs=30, compute_loss=True, report_delay=60*10,\n        callbacks=[EpochSaver(model_save_dir, word2vec_save_name)])\n    time_elapsed = time.time() - since\n    print('Time to train: {:.0f}min {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))","execution_count":7},{"metadata":{"id":"CD65E907B7E74C26892691F078A5EC87","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Loss: 134217728.0\nword2vec kv saved to /home/kesci/work/common/util_models/word2vec_online_concatHorizontal.kv\n","name":"stdout"}],"source":"if regen_word2vec:\n    tmp_model = Word2Vec.load(word2vec_save_file)\n    # 这个Loss是累积的\n    print(\"Loss:\", tmp_model.get_latest_training_loss())\n    tmp_model.wv.save(word2vec_kv_save_file)\n    print(\"word2vec kv saved to {}\".format(word2vec_kv_save_file))\n    del tmp_model\n    gc.collect()","execution_count":12},{"metadata":{"id":"93DDDA439AA94E8492DCC222AE8ACE24","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# tmp_model = Word2Vec.load(word2vec_save_file)\n# print(tmp_model[\"10\"])","execution_count":13},{"metadata":{"id":"675174C5671A4BE288B23E7EBABE3DEB","mdEditEnable":false},"cell_type":"markdown","source":"## 杂项"},{"metadata":{"id":"3627EFE153374DFB856A1F27C89D3D47","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"637\n","name":"stdout"}],"source":"import os\nprint(os.getpid())","execution_count":1},{"metadata":{"id":"6601A99C2A3E42D78F3E45E9929D9801","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"! kill -9 637","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}