{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "本文档是使用tfIdf+lr并随机采样1000万条数据建立的Baseline模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入各种库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是否为线下调试\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 9392\r\n",
      "-rw-r--r--@ 1 niudong  staff   1.7M  5 29 14:28 train_data.csv\r\n",
      "-rw-r--r--@ 1 niudong  staff   1.6M  5 29 15:15 train_processed.csv\r\n",
      "-rw-r--r--@ 1 niudong  staff   282K  5 13 12:17 大数据挑战赛_sample.txt\r\n"
     ]
    }
   ],
   "source": [
    "# 数据集参数\n",
    "if debug:\n",
    "    !ls -lh ../data\n",
    "    data_dir = \"../data/\"\n",
    "    train_data_file = data_dir + \"train_data.csv\"\n",
    "else:\n",
    "    !ls -lh /home/kesci/input/bytedance/first-round/\n",
    "    data_dir = \"/home/kesci/input/bytedance/first-round/\"\n",
    "    train_data_file = data_dir + \"train.csv\"\n",
    "    test_data_file = data_dir + \"test.csv\"\n",
    "# csv的header\n",
    "ori_train_names = [\"query_id\", \"query\", \"query_title_id\", \"title\", \"label\"]\n",
    "ori_test_names = [\"query_id\", \"query\", \"query_title_id\", \"title\"]\n",
    "train_names = [\"feature\", \"label\"]\n",
    "test_names = [\"query_id\", \"query_title_id\", \"feature\"]\n",
    "submit_names = [\"query_id\", \"query_title_id\", \"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算执行某个函数需要的时间\n",
    "class Timer(object):\n",
    "    \"\"\"\n",
    "    Record the consumed time when ran a code block.\n",
    "    \"\"\"\n",
    "    def __init__(self, block_name, prefix=\"----->\"):\n",
    "        self.block_name = block_name\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def __enter__(self):\n",
    "        print(self.prefix+\"Started '\"+self.block_name+\"' block...\")\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        elapsed_time = round(time.time() - self.time_start, 2)\n",
    "        print(self.prefix+\"Finished '\"+self.block_name+\"' block, time used:\", str(elapsed_time)+\"s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取CSV文件\n",
    "def ReadCSV(filename, names, sep=\",\", iterator=True):\n",
    "    # http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv\n",
    "    return pd.read_csv(\n",
    "        filename, \n",
    "        names=names,\n",
    "        sep=sep,\n",
    "        iterator=iterator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量读入数据，并apply处理函数\n",
    "def ProcessChunk(filename, func, names, chunk_size=5000000):\n",
    "    reader = ReadCSV(filename, names)\n",
    "    while True:\n",
    "        try:\n",
    "            print(\"Reading chunk...\")\n",
    "            func(reader.get_chunk(chunk_size))\n",
    "        except StopIteration:\n",
    "            print(\"Finished process.\")\n",
    "            return\n",
    "# def handle(x):\n",
    "#     print(x)\n",
    "#     print()\n",
    "# ProcessChunk(train_data_file, handle, names=ori_train_names, chunk_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 蓄水池读文件, 本方法巨慢无比!\n",
    "def ReservoirSample(filename, count, names=ori_train_names):\n",
    "    print(\"Reservoir sample...\")\n",
    "    reader = ReadCSV(filename, names)\n",
    "    try:\n",
    "        # 对于前面count个元素完全选择\n",
    "        res = reader.get_chunk(count)\n",
    "    except StopIteration:\n",
    "        print(\"Count exceeds the file size.\")\n",
    "        return res\n",
    "    i = count\n",
    "    while True:\n",
    "        try:\n",
    "            i += 1\n",
    "            # https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randint.html\n",
    "            tmp = random.randint(1, i+1)\n",
    "            datapoint = reader.get_chunk(1).iloc[0]\n",
    "            if tmp <= count:\n",
    "                res.iloc[tmp - 1] = datapoint\n",
    "        except StopIteration:\n",
    "            print(\"Finished sample.\")\n",
    "            return res\n",
    "# tmp = ReservoirSample(train_data_file, 5)\n",
    "# print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照rate比例从每个chunk中随机采样样本\n",
    "def RandomSample(filename, rate, chunk_size=1000000, random_state=None, names=ori_train_names):\n",
    "    print(\"Random sample...\")\n",
    "    reader = ReadCSV(filename, names)\n",
    "    chunks = []\n",
    "    while True:\n",
    "        try:\n",
    "            # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html\n",
    "            chunks.append(reader.get_chunk(chunk_size).sample(\n",
    "                n=int(chunk_size*rate), \n",
    "                random_state=random_state)\n",
    "            )\n",
    "        except StopIteration:\n",
    "            print(\"Finished sample.\")\n",
    "            break\n",
    "    # 删除无用引用, https://blog.csdn.net/jiangjiang_jian/article/details/79140742\n",
    "    # for x in locals().keys():\n",
    "    #     if x != \"res\":\n",
    "    #         # 会被放入内存池？\n",
    "    #         del locals()[x]\n",
    "    # gc.collect()\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "# tmp = RandomSample(train_data_file, .5, chunk_size=5)\n",
    "# print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing...\n",
      "Finished process.\n",
      "Saved to the file.\n"
     ]
    }
   ],
   "source": [
    "# 重新组合特征\n",
    "def ProcessFeatures(filename, train=True, names=ori_train_names, datadir=data_dir):\n",
    "    reader = ReadCSV(filename, names)\n",
    "    f = lambda x: x[1] + \" s \" + x[3]\n",
    "    count, chunk_size = 1, 10000000\n",
    "    print(\"Start processing...\")\n",
    "    chunks = []\n",
    "    while True:\n",
    "        try:\n",
    "            chunks.append(reader.get_chunk(chunk_size))\n",
    "        except StopIteration:\n",
    "            print(\"Finished process.\")\n",
    "            break\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    if train:\n",
    "        fname = datadir+\"train_processed.csv\"\n",
    "        pd.DataFrame({\n",
    "            \"feature\": df.apply(f, axis=1), \n",
    "            \"label\": df.label}).to_csv(\n",
    "            fname, header=None, index=None)\n",
    "    else:\n",
    "        fname = datadir+\"test_processed.csv\"\n",
    "        pd.DataFrame({\n",
    "            \"query_id\":df.iloc[:, 0],\n",
    "            \"query_title_id\": df.iloc[:, 2],\n",
    "            \"feature\": df.apply(f, axis=1)\n",
    "        }).to_csv(\n",
    "            fname, header=None, index=None)\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Saved to the file.\")\n",
    "\n",
    "ProcessFeatures(train_data_file)\n",
    "# 线上处理test数据时候需要调用\n",
    "# ProcessFeatures(test_data_file, train=False, names=ori_test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(o):\n",
    "    print(o[:5])\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = data_dir+\"train_processed.csv\"\n",
    "test_file = data_dir+\"test_processed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunk...\n",
      "                                             feature  label\n",
      "0  1427 5661 29788 1427 387 2299 372 22 1586 1025...      1\n",
      "1  1427 5661 29788 1427 387 2299 372 22 1586 1025...      0\n",
      "2  1427 5661 29788 1427 387 2299 372 22 1586 1025...      1\n",
      "3  1427 5661 29788 1427 361 22 1374 279 1196 27 7...      0\n",
      "4  1427 5661 29788 1427 361 22 1374 279 1196 27 7...      1\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niudong/workon_home/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# 看一下训练数据对不对\n",
    "ProcessChunk(train_file, pr, names=train_names, chunk_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看一下测试数据对不对\n",
    "if not debug:\n",
    "    ProcessChunk(test_file, pr, names=test_names, chunk_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample...\n",
      "Finished sample.\n"
     ]
    }
   ],
   "source": [
    "# 随机采样1000万数据\n",
    "if debug:\n",
    "    chunk_size = 1000\n",
    "else:\n",
    "    chunk_size = 5000000\n",
    "train_random_data = RandomSample(train_file, .1, names=train_names, chunk_size=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部采样所有的测试数据集\n",
    "if not debug:\n",
    "    test_data = ReadCSV(test_file, names=test_names, iterator=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_num = len(train_random_data)\n",
    "if not debug:\n",
    "    test_data_num = len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer(\n",
    "    tokenizer=tokenizer, \n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTfidf(docs, train=True):\n",
    "    model = tfidf_model\n",
    "    if train:\n",
    "        X = model.fit_transform(docs)\n",
    "    else:\n",
    "        X = model.transform(docs)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    X = getTfidf(train_random_data.feature)\n",
    "else:\n",
    "    X = getTfidf(pd.concat(\n",
    "                [train_random_data.feature, \n",
    "                test_data.feature], ignore_index=True))\n",
    "train_labels = train_random_data.label.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7824)\t0.4123587542358278\n",
      "  (0, 2151)\t0.10204410261494036\n",
      "  (0, 1375)\t0.2061793771179139\n",
      "  (0, 4966)\t0.11917891334939011\n",
      "  (0, 10419)\t0.028576062600339843\n",
      "  (0, 10012)\t0.3803755226594655\n",
      "  (0, 4492)\t0.2144001980292915\n",
      "  (0, 7957)\t0.14038056135790672\n",
      "  (0, 4001)\t0.17478536358343139\n",
      "  (0, 10066)\t0.19018776132973275\n",
      "  (0, 5775)\t0.0762740017266949\n",
      "  (0, 4705)\t0.2061793771179139\n",
      "  (0, 1120)\t0.3053812818706433\n",
      "  (0, 1869)\t0.22598679434084396\n",
      "  (0, 9312)\t0.22598679434084396\n",
      "  (0, 2542)\t0.22598679434084396\n",
      "  (0, 4471)\t0.09124453063549247\n",
      "  (0, 487)\t0.22598679434084396\n",
      "  (0, 2684)\t0.16840879949221438\n",
      "  (0, 6019)\t0.11884073114682828\n",
      "  (0, 2739)\t0.1998028130266969\n",
      "  (0, 2257)\t0.2061793771179139\n",
      "  (0, 5887)\t0.0763499010079689\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(train_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrTrain(train=True):\n",
    "    \n",
    "    global X\n",
    "    \n",
    "    if train:\n",
    "        times = 1\n",
    "    else:\n",
    "        times = 1\n",
    "    for i in range(times):\n",
    "        \n",
    "        print(\"Prepare features...\")\n",
    "        train_features = X[:train_data_num]\n",
    "        test_features = X[train_data_num:]\n",
    "        \n",
    "        if train:\n",
    "            split_train = int(train_data_num*0.7)\n",
    "            train_X, test_X = train_features[:split_train], train_features[split_train:]\n",
    "            train_y, test_y = train_labels[:split_train], train_labels[split_train:]\n",
    "        else:\n",
    "            train_X, test_X = train_features, test_features\n",
    "            train_y = train_labels\n",
    "            \n",
    "        model = LogisticRegression(C=5, solver=\"liblinear\")\n",
    "        print(\"Fitting model...\")\n",
    "        pred = model.fit(train_X, train_y).predict_proba(test_X)[:, 1]\n",
    "        \n",
    "        if train:\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(test_y, pred, pos_label=1)\n",
    "            res = metrics.auc(fpr, tpr)\n",
    "            print(\"AUC:\", res)\n",
    "        else:\n",
    "            res = []\n",
    "            for i in range(test_data_num):\n",
    "                tmp = test_data.iloc[i]\n",
    "                res.append([tmp[0], tmp[1], pred[i]])\n",
    "            print(res[:10])\n",
    "            pd.DataFrame(np.array(res)).to_csv(\"submit.csv\", index=False, header=None)\n",
    "            \n",
    "    for x in locals().keys():\n",
    "        del locals()[x]\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'lr train' block...\n",
      "Prepare features...\n",
      "Fitting model...\n",
      "AUC: 0.4986074074074075\n",
      "----->Finished 'lr train' block, time used: 0.06s.\n"
     ]
    }
   ],
   "source": [
    "# 执行训练阶段\n",
    "with Timer(\"lr train\"):\n",
    "    lrTrain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提交结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一次执行需要\n",
    "# !wget -nv -O kesci_submit https://www.heywhale.com/kesci_submit&&chmod +x kesci_submit\n",
    "!./kesci_submit -token 490475a1ae106f67 -file submit.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
