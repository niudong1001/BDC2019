{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "def create_train_data():\n",
    "    tmp = pd.DataFrame({\"query_id\":[1, 2, 3, 4, 5, 6,7,8,9,10], \n",
    "                    \"query\": [\"12 32\", \"12 32\", \"12 33 23\", \"12 3\", \"1 32\", \"12 10 32\", \"12 10 32\", \"12 33 32\", \"12 3 12\", \"12 32 11 12\"], \n",
    "                    \"query_title\": [1, 2, 3, 4, 5, 6,7,8,9,10], \n",
    "                    \"title\": [\"12 21\", \" 11 2\", \"1\", \" 6 5 2\", \"7 8 9 10\", '1 2', '2 3', '4 5', '1 1 2 2', '12'], \n",
    "                    \"label\":[1, 0, 0, 0, 1, 1, 1, 0, 0, 1]})\n",
    "    tmp.to_csv(\"train.csv\", header=None, index=None)\n",
    "create_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "def create_test_data():\n",
    "    tmp = pd.DataFrame({\"query_id\":[1, 2, 3, 4, 5, 6,7,8,9,10], \n",
    "                    \"query\": [\"12 32\", \"12 32\", \"12 33 23\", \"12 3\", \"1 32\", \"12 10 32\", \"12 10 32\", \"12 33 32\", \"12 3 12\", \"12 32 11 12\"], \n",
    "                    \"query_title\": [1, 2, 11, 4, 5, 6,7,8,9,10], \n",
    "                    \"title\": [\"12 21\", \" 11 2\", \"1\", \" 6 5 2\", \"7 8 9 10\", '1 2', '2 3', '4 5', '1 1 2 2', '12']\n",
    "                    })\n",
    "    tmp.to_csv(\"test.csv\", header=None, index=None)\n",
    "create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_train_names = [\"query_id\", \"query\", \"query_title_id\", \"title\", \"label\"]\n",
    "ori_test_names = [\"query_id\", \"query\", \"query_title_id\", \"title\"]\n",
    "train_data_file = \"train.csv\"\n",
    "test_data_file = \"test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取CSV文件\n",
    "def ReadCSV(filename, names, sep=\",\", iterator=True):\n",
    "    # http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv\n",
    "    return pd.read_csv(\n",
    "        filename, \n",
    "        names=names,\n",
    "        sep=sep,\n",
    "        iterator=iterator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunk...\n",
      "   query_id     query  query_title_id     title  label\n",
      "0         1     12 32               1     12 21      1\n",
      "1         2     12 32               2      11 2      0\n",
      "2         3  12 33 23               3         1      0\n",
      "3         4      12 3               4     6 5 2      0\n",
      "4         5      1 32               5  7 8 9 10      1\n",
      "\n",
      "Reading chunk...\n",
      "   query_id        query  query_title_id    title  label\n",
      "5         6     12 10 32               6      1 2      1\n",
      "6         7     12 10 32               7      2 3      1\n",
      "7         8     12 33 32               8      4 5      0\n",
      "8         9      12 3 12               9  1 1 2 2      0\n",
      "9        10  12 32 11 12              10       12      1\n",
      "\n",
      "Reading chunk...\n",
      "Finished process.\n"
     ]
    }
   ],
   "source": [
    "# 批量读入数据，并apply处理函数\n",
    "def ProcessChunk(filename, func, names, chunk_size=5000000):\n",
    "    reader = ReadCSV(filename, names)\n",
    "    while True:\n",
    "        try:\n",
    "            print(\"Reading chunk...\")\n",
    "            func(reader.get_chunk(chunk_size))\n",
    "        except StopIteration:\n",
    "            print(\"Finished process.\")\n",
    "            return\n",
    "# def handle(x):\n",
    "#     print(x)\n",
    "#     print()\n",
    "# ProcessChunk(train_data_file, handle, names=ori_train_names, chunk_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reservoir sample...\n",
      "Finished sample.\n",
      "   query_id        query  query_title_id     title  label\n",
      "0        10  12 32 11 12              10        12      1\n",
      "1         9      12 3 12               9   1 1 2 2      0\n",
      "2         8     12 33 32               8       4 5      0\n",
      "3         4         12 3               4     6 5 2      0\n",
      "4         5         1 32               5  7 8 9 10      1\n"
     ]
    }
   ],
   "source": [
    "# 蓄水池读文件, 本方法巨慢无比!\n",
    "def ReservoirSample(filename, count, names=ori_train_names):\n",
    "    print(\"Reservoir sample...\")\n",
    "    reader = ReadCSV(filename, names)\n",
    "    try:\n",
    "        # 对于前面count个元素完全选择\n",
    "        res = reader.get_chunk(count)\n",
    "    except StopIteration:\n",
    "        print(\"Count exceeds the file size.\")\n",
    "        return res\n",
    "    i = count\n",
    "    while True:\n",
    "        try:\n",
    "            i += 1\n",
    "            # https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randint.html\n",
    "            tmp = random.randint(1, i+1)\n",
    "            datapoint = reader.get_chunk(1).iloc[0]\n",
    "            if tmp <= count:\n",
    "                res.iloc[tmp - 1] = datapoint\n",
    "        except StopIteration:\n",
    "            print(\"Finished sample.\")\n",
    "            return res\n",
    "# tmp = ReservoirSample(train_data_file, 5)\n",
    "# print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample...\n",
      "Finished sample.\n",
      "   query_id     query  query_title_id    title  label\n",
      "0         4      12 3               4    6 5 2      0\n",
      "1         1     12 32               1    12 21      1\n",
      "2         6  12 10 32               6      1 2      1\n",
      "3         9   12 3 12               9  1 1 2 2      0\n"
     ]
    }
   ],
   "source": [
    "# 按照rate比例从每个chunk中随机采样样本\n",
    "def RandomSample(filename, rate, chunk_size=1000000, random_state=None, names=ori_train_names):\n",
    "    print(\"Random sample...\")\n",
    "    reader = ReadCSV(filename, names)\n",
    "    chunks = []\n",
    "    while True:\n",
    "        try:\n",
    "            # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html\n",
    "            chunks.append(reader.get_chunk(chunk_size).sample(\n",
    "                n=int(chunk_size*rate), \n",
    "                random_state=random_state)\n",
    "            )\n",
    "        except StopIteration:\n",
    "            print(\"Finished sample.\")\n",
    "            break\n",
    "    # 删除无用引用, https://blog.csdn.net/jiangjiang_jian/article/details/79140742\n",
    "    # for x in locals().keys():\n",
    "    #     if x != \"res\":\n",
    "    #         # 会被放入内存池？\n",
    "    #         del locals()[x]\n",
    "    # gc.collect()\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "# tmp = RandomSample(train_data_file, .5, chunk_size=5)\n",
    "# print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing...\n",
      "Finished process.\n",
      "Saved to the file.\n"
     ]
    }
   ],
   "source": [
    "# 重新组合特征\n",
    "def ProcessFeatures(filename, train=True, names=ori_train_names):\n",
    "    reader = ReadCSV(filename, names)\n",
    "    f = lambda x: x[1] + \" s \" + x[3]\n",
    "    count, chunk_size = 1, 10000000\n",
    "    print(\"Start processing...\")\n",
    "    chunks = []\n",
    "    while True:\n",
    "        try:\n",
    "            chunks.append(reader.get_chunk(chunk_size))\n",
    "        except StopIteration:\n",
    "            print(\"Finished process.\")\n",
    "            break\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    if train:\n",
    "        fname = \"train_processed.csv\"\n",
    "        pd.DataFrame({\n",
    "            \"feature\": df.apply(f, axis=1), \n",
    "            \"label\": df.label}).to_csv(\n",
    "            fname, header=None, index=None)\n",
    "    else:\n",
    "        fname = \"test_processed.csv\"\n",
    "        pd.DataFrame({\n",
    "            \"query_id\":df.iloc[:, 0],\n",
    "            \"query_title_id\": df.iloc[:, 2],\n",
    "            \"feature\": df.apply(f, axis=1)\n",
    "        }).to_csv(\n",
    "            fname, header=None, index=None)\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Saved to the file.\")\n",
    "\n",
    "# ProcessFeatures(train_data_file)\n",
    "ProcessFeatures(test_data_file, train=False, names=ori_test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 按照chunk_size将train.csv文件分割，并组合特征\n",
    "# def ProcessFeatures(filename, chunk_size=10000000, names=[\"query_id\", \"query\", \"query_title_id\", \"title\", \"label\"]):\n",
    "#     print(\"Start processing...\")\n",
    "#     reader = pd.read_csv(\n",
    "#         filename, \n",
    "#         sep=\",\",\n",
    "#         iterator=True,\n",
    "#         names=names\n",
    "#     )\n",
    "#     # reader.get_chunk(1)  # 去除头\n",
    "#     f = lambda x: x[1] + \" s \" + x[3]\n",
    "#     count = 1\n",
    "#     while True:\n",
    "#         try:\n",
    "#             tmp = reader.get_chunk(chunk_size)\n",
    "#             new_csv = pd.DataFrame({\"feature\": tmp.apply(f, axis=1), \"label\": tmp.label})\n",
    "#             new_csv.to_csv(\"train_\"+str(count)+\".csv\", header=None, index=None)\n",
    "#             count += 1\n",
    "#             print(\"Saved to file.\")\n",
    "#         except StopIteration:\n",
    "#             print(\"Finished process.\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 合并分割的数据集\n",
    "# def ConcatFiles(filenum=10):\n",
    "#     chunks = []\n",
    "#     for _ in range(filenum):\n",
    "#         chunks.append(pd.read_csv(\"train_\"+str(_+1)+\".csv\", sep=\",\", names=[\"feature\", \"label\"], header=None))\n",
    "#     df = pd.concat(chunks, ignore_index=True, axis=0)\n",
    "#     df.to_csv(\"train_processed.csv\", header=None, index=None)\n",
    "#     print(\"Concated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 656\r\n",
      "drwxr-xr-x  20 niudong  staff     640  5 28 21:35 \u001b[1m\u001b[36m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  82 niudong  staff    2624  5 27 09:54 \u001b[1m\u001b[36m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 niudong  staff    6148  5 28 16:50 .DS_Store\r\n",
      "drwxr-xr-x   6 niudong  staff     192  5 28 20:43 \u001b[1m\u001b[36m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 niudong  staff   11435  5 28 21:35 copy.ipynb\r\n",
      "drwxr-xr-x   3 niudong  staff      96  5 26 13:10 \u001b[1m\u001b[36mdata\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 niudong  staff    8260  5 28 20:27 feature_engineer.ipynb\r\n",
      "-rw-r--r--   1 niudong  staff   87007  5 27 13:13 foo.csv\r\n",
      "-rw-r--r--   1 niudong  staff  100497  5 27 13:13 pandas_intro.ipynb\r\n",
      "-rw-r--r--   1 niudong  staff   21652  5 27 13:20 preprocess.ipynb\r\n",
      "-rw-r--r--@  1 niudong  staff     174  5 28 21:33 test.csv\r\n",
      "-rw-r--r--@  1 niudong  staff     194  5 28 21:33 test_processed.csv\r\n",
      "-rw-r--r--@  1 niudong  staff     193  5 28 21:27 train.csv\r\n",
      "-rw-r--r--@  1 niudong  staff      32  5 28 20:27 train_1.csv\r\n",
      "-rw-r--r--@  1 niudong  staff      31  5 28 20:27 train_2.csv\r\n",
      "-rw-r--r--@  1 niudong  staff      35  5 28 20:27 train_3.csv\r\n",
      "-rw-r--r--@  1 niudong  staff      34  5 28 20:27 train_4.csv\r\n",
      "-rw-r--r--@  1 niudong  staff      39  5 28 20:27 train_5.csv\r\n",
      "-rw-r--r--@  1 niudong  staff     171  5 28 21:29 train_processed.csv\r\n",
      "-rw-r--r--@  1 niudong  staff     337  5 27 19:50 数据集信息.md\r\n"
     ]
    }
   ],
   "source": [
    "train_file = \"train_processed.csv\"\n",
    "test_file = \"test_processed.csv\"\n",
    "train_names = [\"feature\", \"label\"]\n",
    "test_names = [\"query_id\", \"query_title_id\", \"feature\"]\n",
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunk...\n",
      "   query_id  query_title_id          feature\n",
      "0         1               1    12 32 s 12 21\n",
      "1         2               2    12 32 s  11 2\n",
      "2         3              11     12 33 23 s 1\n",
      "3         4               4    12 3 s  6 5 2\n",
      "4         5               5  1 32 s 7 8 9 10\n",
      "0      12 32 s 12 21\n",
      "1      12 32 s  11 2\n",
      "2       12 33 23 s 1\n",
      "3      12 3 s  6 5 2\n",
      "4    1 32 s 7 8 9 10\n",
      "5                  1\n",
      "6                  2\n",
      "7                  3\n",
      "8                  4\n",
      "9                  5\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niudong/workon_home/py3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def pc(o):\n",
    "    print(o[:5])\n",
    "    # print(pd.concat([o.feature, o.query_id], ignore_index=True))\n",
    "    sys.exit()\n",
    "ProcessChunk(test_file, pc, names=test_names, chunk_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample...\n",
      "Finished sample.\n"
     ]
    }
   ],
   "source": [
    "# 随机采样1000万数据\n",
    "train_random_data = RandomSample(train_file, .5, names=train_names, chunk_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部采样所有的测试数据集\n",
    "test_data = ReadCSV(test_file, names=test_names, iterator=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 10\n",
      "           feature  label\n",
      "0    12 32 s  11 2      0\n",
      "1   12 10 32 s 2 3      1\n",
      "2   12 10 32 s 1 2      1\n",
      "3  1 32 s 7 8 9 10      1\n",
      "4     12 33 23 s 1      0\n",
      "   query_id  query_title_id            feature\n",
      "0         1               1      12 32 s 12 21\n",
      "1         2               2      12 32 s  11 2\n",
      "2         3              11       12 33 23 s 1\n",
      "3         4               4      12 3 s  6 5 2\n",
      "4         5               5    1 32 s 7 8 9 10\n",
      "5         6               6     12 10 32 s 1 2\n",
      "6         7               7     12 10 32 s 2 3\n",
      "7         8               8     12 33 32 s 4 5\n",
      "8         9               9  12 3 12 s 1 1 2 2\n",
      "9        10              10   12 32 11 12 s 12\n"
     ]
    }
   ],
   "source": [
    "train_data_num = len(train_random_data)\n",
    "test_data_num = len(test_data)\n",
    "print(train_data_num, test_data_num)\n",
    "print(train_random_data[:10])\n",
    "print(test_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "  return text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfVectorizer(\n",
    "    tokenizer=tokenizer, \n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTfidf(docs, train=True):\n",
    "    model = tfidf_model\n",
    "    if train:\n",
    "      X = model.fit_transform(docs)\n",
    "    else:\n",
    "      X = model.transform(docs)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = getTfidf(pd.concat(\n",
    "            [train_random_data.feature, \n",
    "            test_data.feature], ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_random_data.label.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrTrain(train=True):\n",
    "    \n",
    "    global X\n",
    "    \n",
    "    if train:\n",
    "        times = 1\n",
    "    else:\n",
    "        times = 1\n",
    "    for i in range(times):\n",
    "        \n",
    "        print(\"Prepare features...\")\n",
    "        train_features = X[:train_data_num]\n",
    "        test_features = X[train_data_num:]\n",
    "        \n",
    "        if train:\n",
    "            split_train = int(train_data_num*0.7)\n",
    "            train_X, test_X = train_features[:split_train], train_features[split_train:]\n",
    "            train_y, test_y = train_labels[:split_train], train_labels[split_train:]\n",
    "        else:\n",
    "            train_X, test_X = train_features, test_features\n",
    "            train_y = train_labels\n",
    "            \n",
    "        model = LogisticRegression(C=5, solver=\"liblinear\")\n",
    "        print(\"Fitting model...\")\n",
    "        pred = model.fit(train_X, train_y).predict_proba(test_X)[:, 1]\n",
    "        \n",
    "        if train:\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(test_y, pred, pos_label=1)\n",
    "            res = metrics.auc(fpr, tpr)\n",
    "            print(\"AUC:\", res)\n",
    "        else:\n",
    "            res = []\n",
    "            for i in range(test_data_num):\n",
    "                tmp = test_data.iloc[i]\n",
    "                res.append([tmp[0], tmp[1], pred[i]])\n",
    "            print(res[:10])\n",
    "            pd.DataFrame(np.array(res)).to_csv(\"submit.csv\", index=False, header=None)\n",
    "            \n",
    "    for x in locals().keys():\n",
    "        del locals()[x]\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare features...\n",
      "Fitting model...\n",
      "[[1, 1, 0.5373475065168729], [2, 2, 0.3255105761308011], [3, 11, 0.2905469781255423], [4, 4, 0.49323606496352046], [5, 5, 0.8063067508395423], [6, 6, 0.7699538884323154], [7, 7, 0.7940174402020639], [8, 8, 0.44701794061582345], [9, 9, 0.6411714345296955], [10, 10, 0.40459781503015]]\n",
      "Cost time: 0.04454302787780762\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lrTrain(False)\n",
    "end_time = time.time()\n",
    "print(\"Cost time:\", end_time-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
