{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from nltk.util import ngrams\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the consumed time when ran a code block.\n",
    "class Timer(object):\n",
    "    def __init__(self, block_name, prefix=\"----->\"):\n",
    "        self.block_name = block_name\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def __enter__(self):\n",
    "        print(self.prefix+\"Started '\"+self.block_name+\"' block...\")\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        elapsed_time = round(time.time() - self.time_start, 2)\n",
    "        print(self.prefix+\"Finished '\"+self.block_name+\"' block, time used:\", str(elapsed_time)+\"s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_set_ratio(a, b):\n",
    "    return Levenshtein.setratio(a.split(), b.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_set_ratio1(a, b):\n",
    "    return Levenshtein.setratio(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'veced' block...\n",
      "[0.77777778 0.77777778 0.77777778 ... 0.77777778 0.77777778 0.77777778]\n",
      "----->Finished 'veced' block, time used: 14.3s.\n"
     ]
    }
   ],
   "source": [
    "a = [\"he she me\"] * 5000000\n",
    "b = [\"she hate he\"] * 5000000\n",
    "# print(edit_set_ratio(a[0], b[0]))\n",
    "# print(edit_set_ratio(a[1], b[1]))\n",
    "with Timer(\"veced\"):\n",
    "    print(np.vectorize(edit_set_ratio)(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'non-veced' block...\n",
      "----->Finished 'non-veced' block, time used: 2.05s.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c5f03622edfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m })\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"non-veced\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0medit_set_ratio1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workon_home/py3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6485\u001b[0m                          \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6486\u001b[0m                          kwds=kwds)\n\u001b[0;32m-> 6487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workon_home/py3/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workon_home/py3/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m                                           \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                                           \u001b[0mdummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                                           labels=labels)\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.reduce\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c5f03622edfb>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m })\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"non-veced\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0medit_set_ratio1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workon_home/py3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workon_home/py3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4370\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4372\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'getitem'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n",
      "\u001b[0;32m~/workon_home/py3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_convert_scalar_indexer\u001b[0;34m(self, key, kind)\u001b[0m\n\u001b[1;32m   2849\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_index_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_convert_scalar_indexer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'getitem'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iloc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'iloc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = pd.DataFrame({\n",
    "    \"a\": [[\"he\", \"she\", \"me\"]] * 5000000,\n",
    "    \"b\": [[\"he\", \"she\", \"me\"]] * 5000000\n",
    "})\n",
    "# with Timer(\"non-veced\"):\n",
    "#     print(t.apply(lambda x: edit_set_ratio1(x[\"a\"], x[\"b\"]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6]\n",
    "print(list(map(max, a, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unigrams(words):\n",
    "    \"\"\"\n",
    "        Input: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "        Output: a list of unigram\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    return words\n",
    "\n",
    "# One line time: 5.04 µs ± 272 ns\n",
    "def _bigrams(words, join_string='_', skip=0):\n",
    "    \"\"\"\n",
    "      Input: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "      Output: a list of bigram, e.g., [\"I_am\", \"am_Denny\"]\n",
    "      I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "        lst = []\n",
    "        for i in range(L-1):\n",
    "            for k in range(1,skip+2):\n",
    "                if i+k < L:\n",
    "                    lst.append(join_string.join([words[i], words[i+k]]) )\n",
    "    else:\n",
    "        # set it as unigram\n",
    "        lst = _unigrams(words)\n",
    "    return lst\n",
    "\n",
    "\n",
    "# One line time: 5.89 µs ± 196 ns\n",
    "def _trigrams(words, join_string='_', skip=0):\n",
    "    \"\"\"\n",
    "      Input: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "      Output: a list of trigram, e.g., [\"I_am_Denny\"]\n",
    "      I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in range(L-2):\n",
    "            for k1 in range(1,skip+2):\n",
    "                for k2 in range(1,skip+2):\n",
    "                    if i+k1 < L and i+k1+k2 < L:\n",
    "                        lst.append(join_string.join([words[i], words[i+k1], words[i+k1+k2]]) )\n",
    "    else:\n",
    "        # set it as bigram\n",
    "        lst = _bigrams(words, join_string, skip)\n",
    "    return lst\n",
    "\n",
    "def unichars(text):\n",
    "    return _unigrams(list(text))\n",
    "\n",
    "def bichars(text):\n",
    "    return _bigrams(list(text))\n",
    "\n",
    "def trichars(text):\n",
    "    return _trigrams(list(text))\n",
    "\n",
    "def unigrams(text):\n",
    "    return _unigrams(text.split())\n",
    "\n",
    "def bigrams(text):\n",
    "    return _bigrams(text.split())\n",
    "\n",
    "def trigrams(text):\n",
    "    return _trigrams(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_func(func):\n",
    "    def do_it(arr):\n",
    "        return [func(_) for _ in arr]\n",
    "    return do_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"q_ngram\": [[\"1\", \"2\", \"3\"]]*100000,\n",
    "    \"t_ngram\": [[\"1\", \"4\", \"2\", \"5\"]]*100000,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"pandas apply\"):\n",
    "    df['q-len'] = df['a'].apply(lambda x: len(x))\n",
    "with Timer(\"vec\"):\n",
    "    df['q-len'] = vectorize_func(len)(df[\"a\"])\n",
    "with Timer(\"np vec\"):\n",
    "    df['q-len'] = np.vectorize(len)(df[\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'extract ngram length' block...\n",
      "----->Finished 'extract ngram length' block, time used: 3.01s.\n"
     ]
    }
   ],
   "source": [
    "prefix = \"test\"\n",
    "with Timer(\"extract ngram length\"):\n",
    "    df['%s_q-len' % prefix] = np.vectorize(len)(df[\"q_ngram\"])\n",
    "    df['%s_t-len' % prefix] = np.vectorize(len)(df[\"t_ngram\"])\n",
    "    df['%s_q-log-len' % prefix] = np.vectorize(lambda x: np.log1p(len(x)))(df[\"q_ngram\"])\n",
    "    df['%s_t-log-len' % prefix] = np.vectorize(lambda x: np.log1p(len(x)))(df[\"t_ngram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     q_ngram       t_ngram  test_q-len  test_t-len  test_q-log-len  \\\n",
      "0  [a, b, c]  [a, b, c, d]           3           4        1.386294   \n",
      "1  [a, b, c]  [a, b, c, d]           3           4        1.386294   \n",
      "\n",
      "   test_t-log-len  \n",
      "0        1.609438  \n",
      "1        1.609438  \n"
     ]
    }
   ],
   "source": [
    "print(df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'extract ngram length' block...\n",
      "----->Finished 'extract ngram length' block, time used: 3.69s.\n"
     ]
    }
   ],
   "source": [
    "prefix = \"test\"\n",
    "with Timer(\"extract ngram length\"):\n",
    "    df['%s_q-len' % prefix] = df['q_ngram'].apply(lambda x: len(x))\n",
    "    df['%s_t-len' % prefix] = df['t_ngram'].apply(lambda x: len(x))\n",
    "    df['%s_q-log-len' % prefix] = df['q_ngram'].apply(lambda x: np.log1p(len(x)))\n",
    "    df['%s_t-log-len' % prefix] = df['t_ngram'].apply(lambda x: np.log1p(len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     q_ngram       t_ngram  test_q-len  test_t-len  test_q-log-len  \\\n",
      "0  [a, b, c]  [a, b, c, d]           3           4        1.386294   \n",
      "1  [a, b, c]  [a, b, c, d]           3           4        1.386294   \n",
      "\n",
      "   test_t-log-len  \n",
      "0        1.609438  \n",
      "1        1.609438  \n"
     ]
    }
   ],
   "source": [
    "print(df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def try_divide(x, y):\n",
    "    \"\"\"try to divide two numbers\"\"\"\n",
    "    return float(x) / y if y != 0.0 else 0.0\n",
    "print(try_divide(10, 0))\n",
    "def jaccard_ratio(a, b):\n",
    "    a, b = set(a), set(b)\n",
    "    c = a & b\n",
    "    return try_divide(float(len(c)), (len(a) + len(b) - len(c)))\n",
    "def dice_ratio(a, b):\n",
    "    a, b = set(a), set(b)\n",
    "    return try_divide(2 * len(a & b), len(a) + len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_ratio(a, b):\n",
    "    a, b = set(a), set(b)\n",
    "    c = a & b\n",
    "    return try_divide(len(c), len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_ratio1(a, b):\n",
    "    a, b = set(a), set(b)\n",
    "    c = a & b\n",
    "    return try_divide(float(len(c)), (len(a) + len(b) - len(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice_ratio1 = lambda x, y: try_divide(2 * len(set(a)&set(b)), (len(set(a)) + len(set(b))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "print(jaccard_ratio1([\"i\", \"hate\", \"him\"], [\"she\", \"hate\", \"me\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'similay' block...\n",
      "----->Finished 'similay' block, time used: 3.92s.\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"similay\"):\n",
    "    df['%s_dice-ratio'% prefix] = df.apply(lambda x: jaccard_ratio(x['q_ngram'], x['t_ngram']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     q_ngram       t_ngram  test_dice-ratio\n",
      "0  [a, b, c]  [a, b, c, d]             0.75\n",
      "1  [a, b, c]  [a, b, c, d]             0.75\n"
     ]
    }
   ],
   "source": [
    "print(df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'similay' block...\n",
      "----->Finished 'similay' block, time used: 0.14s.\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"similay\"):\n",
    "    df['%s_dice-ratio'% prefix] = np.vectorize(dice_ratio)(df['q_ngram'], df['t_ngram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     q_ngram       t_ngram  test_dice-ratio\n",
      "0  [a, b, c]  [a, b, c, d]             0.75\n",
      "1  [a, b, c]  [a, b, c, d]             0.75\n"
     ]
    }
   ],
   "source": [
    "print(df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niudong/workon_home/py3/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load(\"./stage1/output/word2vec.kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "vector_size = model.vectors.shape[1]\n",
    "print(vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def wmd(s1, s2):\n",
    "    # 从文档整体上来考虑两个文档之间的相似性，这种技术称为词移距离（WMD）,https://blog.csdn.net/qrlhl/article/details/78512598\n",
    "    # https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance\n",
    "    dis = np.nan_to_num(model.wmdistance(s1, s2))\n",
    "    return dis if dis < 100 else 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'wmd' block...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-190-b93a11fadb36>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"wmd\" failed type inference due to: Untyped global name 'model': cannot determine Numba type of <class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n",
      "\n",
      "File \"<ipython-input-190-b93a11fadb36>\", line 5:\n",
      "def wmd(s1, s2):\n",
      "    <source elided>\n",
      "    # https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance\n",
      "    dis = np.nan_to_num(model.wmdistance(s1, s2))\n",
      "    ^\n",
      "\n",
      "  @numba.jit\n",
      "/Users/niudong/workon_home/py3/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function \"wmd\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"<ipython-input-190-b93a11fadb36>\", line 2:\n",
      "@numba.jit\n",
      "def wmd(s1, s2):\n",
      "^\n",
      "\n",
      "  self.func_ir.loc))\n",
      "/Users/niudong/workon_home/py3/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-190-b93a11fadb36>\", line 2:\n",
      "@numba.jit\n",
      "def wmd(s1, s2):\n",
      "^\n",
      "\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Finished 'wmd' block, time used: 45.67s.\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"wmd\"):\n",
    "    df['%s_wmd'% prefix] = df.apply(lambda x: wmd(x['q_ngram'], x['t_ngram']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     q_ngram       t_ngram  test_wmd\n",
      "0  [1, 2, 3]  [1, 4, 2, 5]   0.02527\n",
      "1  [1, 2, 3]  [1, 4, 2, 5]   0.02527\n"
     ]
    }
   ],
   "source": [
    "print(df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'wmd' block...\n",
      "----->Finished 'wmd' block, time used: 41.16s.\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"wmd\"):\n",
    "    df['%s_wmd1'% prefix] = np.vectorize(wmd)(df['q_ngram'], df['t_ngram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     q_ngram       t_ngram  test_wmd  test_wmd1\n",
      "0  [1, 2, 3]  [1, 4, 2, 5]   0.02527    0.02527\n",
      "1  [1, 2, 3]  [1, 4, 2, 5]   0.02527    0.02527\n"
     ]
    }
   ],
   "source": [
    "print(df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(words):\n",
    "    # 计算embedding均值\n",
    "    global vector_size\n",
    "    # assert isinstance(words, list)\n",
    "    return np.nan_to_num(\n",
    "        np.array([\n",
    "            np.zeros(vector_size)]+[model[w] for w in words if w in model]\n",
    "        ).mean(axis=0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'wmd' block...\n",
      "----->Finished 'wmd' block, time used: 9.22s.\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"wmd\"):\n",
    "    df['q_sen_vec'] = df[\"q_ngram\"].apply(sent2vec)\n",
    "    df['t_sen_vec'] = df[\"t_ngram\"].apply(sent2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     q_ngram       t_ngram  test_wmd  test_wmd1  \\\n",
      "0  [1, 2, 3]  [1, 4, 2, 5]   0.02527    0.02527   \n",
      "1  [1, 2, 3]  [1, 4, 2, 5]   0.02527    0.02527   \n",
      "\n",
      "                                           q_sen_vec  \\\n",
      "0  [-0.003970941637817305, 0.0014471168105956167,...   \n",
      "1  [-0.003970941637817305, 0.0014471168105956167,...   \n",
      "\n",
      "                                           t_sen_vec  \n",
      "0  [-0.0023434016096871347, 0.0011191037250682712...  \n",
      "1  [-0.0023434016096871347, 0.0011191037250682712...  \n"
     ]
    }
   ],
   "source": [
    "print(df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'skew' block...\n",
      "----->Finished 'skew' block, time used: 21.92s.\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"skew\"):\n",
    "    df['%s_q-skew' % prefix] = df['q_sen_vec'].apply(lambda x :skew(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     q_ngram       t_ngram  test_wmd  test_wmd1  \\\n",
      "0  [1, 2, 3]  [1, 4, 2, 5]   0.02527    0.02527   \n",
      "1  [1, 2, 3]  [1, 4, 2, 5]   0.02527    0.02527   \n",
      "\n",
      "                                           q_sen_vec  \\\n",
      "0  [-0.003970941637817305, 0.0014471168105956167,...   \n",
      "1  [-0.003970941637817305, 0.0014471168105956167,...   \n",
      "\n",
      "                                           t_sen_vec  test_q-skew  \n",
      "0  [-0.0023434016096871347, 0.0011191037250682712...    -0.188391  \n",
      "1  [-0.0023434016096871347, 0.0011191037250682712...    -0.188391  \n"
     ]
    }
   ],
   "source": [
    "print(df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'skew' block...\n",
      "----->Finished 'skew' block, time used: 18.96s.\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"skew\"):\n",
    "    df['%s_q-skew1' % prefix] = np.vectorize(skew)(df['q_sen_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     q_ngram       t_ngram  test_wmd  test_wmd1  \\\n",
      "0  [1, 2, 3]  [1, 4, 2, 5]   0.02527    0.02527   \n",
      "1  [1, 2, 3]  [1, 4, 2, 5]   0.02527    0.02527   \n",
      "\n",
      "                                           q_sen_vec  \\\n",
      "0  [-0.003970941637817305, 0.0014471168105956167,...   \n",
      "1  [-0.003970941637817305, 0.0014471168105956167,...   \n",
      "\n",
      "                                           t_sen_vec  test_q-skew  \\\n",
      "0  [-0.0023434016096871347, 0.0011191037250682712...    -0.188391   \n",
      "1  [-0.0023434016096871347, 0.0011191037250682712...    -0.188391   \n",
      "\n",
      "   test_q-skew1  \n",
      "0     -0.188391  \n",
      "1     -0.188391  \n"
     ]
    }
   ],
   "source": [
    "print(df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'kurtosis' block...\n",
      "----->Finished 'kurtosis' block, time used: 28.63s.\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"kurtosis\"):\n",
    "        df['%s_q-kurtosis' % prefix] = df['q_sen_vec'].apply(lambda x :kurtosis(x))\n",
    "        df['%s_t-kurtosis' % prefix] = df['t_sen_vec'].apply(lambda x :kurtosis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     q_ngram       t_ngram  test_wmd  test_wmd1  \\\n",
      "0  [1, 2, 3]  [1, 4, 2, 5]   0.02527    0.02527   \n",
      "1  [1, 2, 3]  [1, 4, 2, 5]   0.02527    0.02527   \n",
      "\n",
      "                                           q_sen_vec  \\\n",
      "0  [-0.003970941637817305, 0.0014471168105956167,...   \n",
      "1  [-0.003970941637817305, 0.0014471168105956167,...   \n",
      "\n",
      "                                           t_sen_vec  test_q-skew  \\\n",
      "0  [-0.0023434016096871347, 0.0011191037250682712...    -0.188391   \n",
      "1  [-0.0023434016096871347, 0.0011191037250682712...    -0.188391   \n",
      "\n",
      "   test_q-skew1  test_q-kurtosis  test_t-kurtosis  \n",
      "0     -0.188391        -0.129808        -0.303393  \n",
      "1     -0.188391        -0.129808        -0.303393  \n"
     ]
    }
   ],
   "source": [
    "print(df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'kurtosis' block...\n",
      "----->Finished 'kurtosis' block, time used: 29.27s.\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"kurtosis\"):\n",
    "        df['%s_q-kurtosis1' % prefix] = np.vectorize(kurtosis)(df['q_sen_vec'])\n",
    "        df['%s_t-kurtosis1' % prefix] = np.vectorize(kurtosis)(df['t_sen_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"kurtosis\"):\n",
    "        df['%s_q-kurtosis1' % prefix] = [kurtosis(_) for _ in df['q_sen_vec']]\n",
    "        df['%s_t-kurtosis1' % prefix] = [kurtosis(_) for _ in df['t_sen_vec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     q_ngram       t_ngram  test_wmd  test_wmd1  \\\n",
      "0  [1, 2, 3]  [1, 4, 2, 5]   0.02527    0.02527   \n",
      "1  [1, 2, 3]  [1, 4, 2, 5]   0.02527    0.02527   \n",
      "\n",
      "                                           q_sen_vec  \\\n",
      "0  [-0.003970941637817305, 0.0014471168105956167,...   \n",
      "1  [-0.003970941637817305, 0.0014471168105956167,...   \n",
      "\n",
      "                                           t_sen_vec  test_q-skew  \\\n",
      "0  [-0.0023434016096871347, 0.0011191037250682712...    -0.188391   \n",
      "1  [-0.0023434016096871347, 0.0011191037250682712...    -0.188391   \n",
      "\n",
      "   test_q-skew1  test_q-kurtosis  test_t-kurtosis  test_q-kurtosis1  \\\n",
      "0     -0.188391        -0.129808        -0.303393         -0.129808   \n",
      "1     -0.188391        -0.129808        -0.303393         -0.129808   \n",
      "\n",
      "   test_t-kurtosis1  \n",
      "0         -0.303393  \n",
      "1         -0.303393  \n"
     ]
    }
   ],
   "source": [
    "print(df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine, jaccard, cityblock, canberra, euclidean, minkowski, braycurtis, mahalanobis\n",
    "\n",
    "def cosine_distance(a, b):\n",
    "    s = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "    return np.nan_to_num(np.dot(a, b)/s) if s != 0.0 else 0.0\n",
    "\n",
    "\n",
    "def jaccard_distance(a, b):\n",
    "    return np.nan_to_num(jaccard(a, b))\n",
    "\n",
    "\n",
    "def braycurtis_distance(a, b):\n",
    "    return np.nan_to_num(braycurtis(a, b))\n",
    "\n",
    "\n",
    "def canberra_distance(a, b):\n",
    "    return np.nan_to_num(canberra(a, b))\n",
    "\n",
    "\n",
    "def cityblock_distance(a, b):\n",
    "    return np.nan_to_num(cityblock(a, b))\n",
    "\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.nan_to_num(euclidean(a, b))\n",
    "\n",
    "\n",
    "def minkowski_distance(a, b, p=1):\n",
    "    return np.nan_to_num(minkowski(a, b, p))\n",
    "\n",
    "def mahalanobis_distance(a, b):\n",
    "    return np.nan_to_num(mahalanobis(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'extract embed distance' block...\n",
      "----->Finished 'extract embed distance' block, time used: 67.9s.\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"extract embed distance\"):\n",
    "        df['%s_cosine-distance' % prefix] = df.apply(lambda x: cosine_distance(x[\"q_sen_vec\"], x[\"t_sen_vec\"]), axis=1)\n",
    "        df['%s_jaccard-distance' % prefix] = df.apply(lambda x: jaccard_distance(x[\"q_sen_vec\"], x[\"t_sen_vec\"]), axis=1)\n",
    "        df['%s_canberra-distance' % prefix] = df.apply(lambda x: canberra_distance(x[\"q_sen_vec\"], x[\"t_sen_vec\"]), axis=1)\n",
    "        df['%s_cityblock-distance' % prefix] = df.apply(lambda x: cityblock_distance(x[\"q_sen_vec\"], x[\"t_sen_vec\"]), axis=1)\n",
    "        df['%s_euclidean-distance' % prefix] = df.apply(lambda x: euclidean_distance(x[\"q_sen_vec\"], x[\"t_sen_vec\"]), axis=1)\n",
    "        df['%s_minkowski-distance' % prefix] = df.apply(lambda x: minkowski_distance(x[\"q_sen_vec\"], x[\"t_sen_vec\"]), axis=1)\n",
    "        df['%s_braycurtis-distance' % prefix] = df.apply(lambda x: braycurtis_distance(x[\"q_sen_vec\"], x[\"t_sen_vec\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'extract embed distance' block...\n",
      "----->Finished 'extract embed distance' block, time used: 25.11s.\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"extract embed distance\"):\n",
    "    df['%s_cosine-distance' % prefix] = np.vectorize(cosine_distance)(df[\"q_sen_vec\"], df[\"t_sen_vec\"])\n",
    "    df['%s_cityblock-distance' % prefix] = np.vectorize(cityblock_distance)(df[\"q_sen_vec\"], df[\"t_sen_vec\"])\n",
    "    df['%s_euclidean-distance' % prefix] = np.vectorize(euclidean_distance)(df[\"q_sen_vec\"], df[\"t_sen_vec\"])\n",
    "    df['%s_jaccard-distance' % prefix] = np.vectorize(jaccard_distance)(df[\"q_sen_vec\"], df[\"t_sen_vec\"])\n",
    "    df['%s_canberra-distance' % prefix] = np.vectorize(canberra_distance)(df[\"q_sen_vec\"], df[\"t_sen_vec\"])\n",
    "    df['%s_braycurtis-distance' % prefix] = np.vectorize(braycurtis_distance)(df[\"q_sen_vec\"], df[\"t_sen_vec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.8000000e+01  1.8000000e+01  3.6635616e+00  2.9444390e+00\n",
      "   1.2244898e-01  6.5217390e-02  2.9920635e-01  4.4801587e-01\n",
      "   3.7000000e+01  1.7000000e+01  3.6375860e+00  2.8903718e+00\n",
      "   3.7037037e-02  1.8867925e-02  2.9472348e-01  3.8540572e-01\n",
      "   3.6000000e+01  1.6000000e+01  3.6109178e+00  2.8332133e+00\n",
      "   0.0000000e+00  0.0000000e+00  2.9030600e-01  3.4875760e-01\n",
      "   3.3169815e-01 -1.7374037e-01 -1.7004882e-01 -1.4810610e-01\n",
      "  -1.5124854e-01  9.9992895e-01  2.2771935e+00  1.9882844e-01\n",
      "   1.0000000e+00  3.8897835e+01  1.9244862e-01]\n",
      " [ 3.8000000e+01  1.5000000e+01  3.6635616e+00  2.7725887e+00\n",
      "   2.5000000e-01  1.4285715e-01  3.2680443e-01  4.3620846e-01\n",
      "   3.7000000e+01  1.4000000e+01  3.6375860e+00  2.7080503e+00\n",
      "   7.8431375e-02  4.0816326e-02  3.2240057e-01  3.8752028e-01\n",
      "   3.6000000e+01  1.3000000e+01  3.6109178e+00  2.6390574e+00\n",
      "   0.0000000e+00  0.0000000e+00  3.0972785e-01  3.6288175e-01\n",
      "   1.3068473e-01 -1.7374037e-01 -1.7450495e-01 -1.4810610e-01\n",
      "  -1.3898596e-01  9.9990946e-01  4.2935538e-01  3.7642054e-02\n",
      "   1.0000000e+00  1.0517207e+01  4.3009013e-02]]\n"
     ]
    }
   ],
   "source": [
    "a = np.load(\"stage2/input/train_v1_feature_concat.npy\")\n",
    "print(a[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2] [5, 6]\n",
      "[3, 4] [7, 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a, b):\n",
    "    print(a, b)\n",
    "res1 = [[1, 2], [3, 4]]\n",
    "res2 = [[5, 6], [7, 8]]\n",
    "list(map(f, res1, res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a = pd.DataFrame()\n",
    "if type(a) == pd.core.frame.DataFrame:\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist, braycurtis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "braycurtis\n",
      "[0.038461538461538464, 0.3333333333333333]\n"
     ]
    }
   ],
   "source": [
    "print(braycurtis.__name__)\n",
    "a = [[0.4, 2, 3], [2, 3, 4]]\n",
    "b = [[0, 2, 3], [5, 6, 7]]\n",
    "Y = list(map(braycurtis, a, b))\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "fuzz.token_set_ratio([\"a\", \"b\", \"c\"], [\"d\", \"e\", \"f\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = {\n",
    "    \"1\" : [1, 2, 3],\n",
    "    \"2\": [4, 5, 6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(text):\n",
    "    # 计算embedding均值\n",
    "    global word2vec_model\n",
    "    return np.nan_to_num(\n",
    "          np.array([word2vec_model[w] for w in text.split() if w in word2vec_model]\n",
    "          ).mean(axis=0)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2.5, 3.5, 4.5]), array([2.5, 3.5, 4.5])]\n"
     ]
    }
   ],
   "source": [
    "sent2vec(\" 1 2 \")\n",
    "tmp = list(map(sent2vec,([\"1 2\", \"2 1\"])))\n",
    "print(tmp)\n",
    "np.save(\"tmp.npy\", tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print([] or [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"query_id\": [1, 1, 2, 2, 2, 3, 3],\n",
    "    \"title\": [\"1 2 3\", \"4 2 3\", '3 3 2', '4 5 6', '5 5 3', '1 2 4', '4 5 6']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDiff(arr, ngram):\n",
    "    set_list = list(map(lambda x: set(ngram(x)), arr))\n",
    "    res = []\n",
    "    for i, _ in enumerate(set_list):\n",
    "        result = _\n",
    "        for __ in set_list:\n",
    "            if _ != __:\n",
    "                result = result.difference(__)\n",
    "        res.append(list(result))\n",
    "    # print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_diff_set = np.frompyfunc(f, \n",
    "    2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_diff_set_group(arr):\n",
    "    return list(reduce(lambda a, b: a.difference(b), arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_diff_set(group):\n",
    "    df = group.apply(lambda x: set(x.split())).values\n",
    "    res = []\n",
    "    for i in range(len(df)):\n",
    "        if i == 0:\n",
    "            res.append(reduce_diff_set_group(df))\n",
    "        else:\n",
    "            tmp = np.copy(df)\n",
    "            tmp = tmp[np.newaxis, :]\n",
    "            tmp[:, [0, i]] = tmp[:, [i, 0]]\n",
    "            tmp = tmp.squeeze()\n",
    "            res.append(reduce_diff_set_group(tmp))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1'], ['4'], ['2'], ['4', '6'], [], ['1', '2'], ['5', '6']]\n"
     ]
    }
   ],
   "source": [
    "grouped = df.groupby(\"query_id\")[\"title\"].agg(reduce_diff_set)\n",
    "res = []\n",
    "for _ in grouped:\n",
    "    res.extend(_)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalQAuc(df):\n",
    "    \n",
    "    auc_score = []\n",
    "\n",
    "    for name, group in df.groupby('query_id'):\n",
    "        try:\n",
    "            auc_score.append(roc_auc_score(group[\"label\"], group[\"prediction\"]\n",
    "                                       ))\n",
    "        except:\n",
    "            auc_score.append(0.5) \n",
    "\n",
    "    return np.mean(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.5, 0.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CalQAuc(pd.DataFrame({\n",
    "    \"query_id\": [1, 1, 2, 2, 3],\n",
    "    \"label\": [1, 0, 1, 1, 0],\n",
    "    \"prediction\": [0.3, 0.5, 0.2, 0.1, 0.6] \n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tmp1\n",
      "0     1\n",
      "1     2\n",
      "2     0\n",
      "3     0\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"tmp\":[0, 0.0, 0, 0.0],\n",
    "    \"tmp1\": [1, 2, 0, 0]\n",
    "})\n",
    "\n",
    "df = df.loc[:,~((df==0.0).all())]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_divide(x, y):\n",
    "    \"\"\"try to divide two numbers\"\"\"\n",
    "    return float(x) / y if y != 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One line time: 4.2 µs ± 109 ns\n",
    "def dice_ratio(a, b):\n",
    "    a, b = set(a), set(b)\n",
    "    return try_divide(2 * len(a & b), len(a) + len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(dice_ratio([\"a\", \"b\", \"c\"], \"acd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "print(Levenshtein.ratio('acf e', 'de f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "print(Levenshtein.seqratio([\"a\", \"c\",  \"f\",\" \", \"e\"], [\"d\",\"e\",\" \", \"f\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('Thorkel', 'Thorgier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio([\"89\",\"9\", \"8\"], [\"1\", \"2\", \"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 2.5, 3.5])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([\n",
    "    [1, 2, 3],\n",
    "    [2, 3, 4]\n",
    "]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "A = csr_matrix([4, 0, 5])\n",
    "B = csr_matrix([1, 0, 3])\n",
    "print(A.multiply(B).sum())\n",
    "print(A.power(2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9, 12, 15],\n",
       "       [21, 24, 27],\n",
       "       [39, 48, 57]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.dot(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_jaccard_sim(x, y):\n",
    "    xy = x.multiply(y).sum()\n",
    "    x_norm = x.power(2).sum()\n",
    "    y_norm = y.power(2).sum()\n",
    "    return try_divide(xy, math.sqrt(x_norm)+math.sqrt(y_norm)-xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = csr_matrix([[1, 2, 3], [4, 5, 6]], shape=(2, 3))\n",
    "B = csr_matrix([[1, 2, 4], [4, 5, 7]], shape=(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import paired_distances\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_divide(x, y):\n",
    "    \"\"\"try to divide two numbers\"\"\"\n",
    "    return float(x) / y if y != 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.9594809496552812, -1.2820868764564912]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(csr_jaccard_sim, A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6]\n",
      " [15]]\n"
     ]
    }
   ],
   "source": [
    "A = csr_matrix([[1, 2, 3], [4, 5, 6]], shape=(2, 3))\n",
    "print(A.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a  c\n",
      "0  [1]  1\n",
      "1  [2]  2\n",
      "2  [3]  3\n"
     ]
    }
   ],
   "source": [
    "tmp = pd.DataFrame({\n",
    "    \"a\": [[1], [2], [3]],\n",
    "    \"c\": [1, 2, 3]\n",
    "})\n",
    "print(tmp.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 16, 21])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([np.array([1, 2, 3])*3, np.array([4, 5, 6])*2]).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.9176629354822472, 0.2601469382930058)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print(scipy.stats.pearsonr([1, 2, 1], [4, 1, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16)\n",
      "[[-0.57735027  0.          0.          0.          0.          0.\n",
      "   0.          0.         -0.57735027  0.          0.          0.\n",
      "   0.          0.57735027  0.          0.        ]\n",
      " [-0.81649658  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.40824829\n",
      "   0.          0.40824829  0.          0.        ]\n",
      " [ 0.          0.          0.          0.         -0.70710678  0.70710678\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.57735027  0.          0.          0.          0.          0.\n",
      "   0.          0.         -0.57735027  0.          0.          0.\n",
      "   0.          0.57735027  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = HashingVectorizer(n_features=2**4)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simhash import Simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(Simhash(['aa', \"cc\"]).distance(Simhash(['bb', \"dd\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "['1']\n",
      "['1', '4']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from simhash import Simhash, SimhashIndex\n",
    "def get_features(s):\n",
    "    width = 3\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^\\w]+', '', s)\n",
    "    return [s[i:i + width] for i in range(max(len(s) - width + 1, 1))]\n",
    "\n",
    "data = {\n",
    "    1: u'How are you? I Am fine. blar blar blar blar blar Thanks.',\n",
    "    2: u'How are you i am fine. blar blar blar blar blar than',\n",
    "    3: u'This is simhash test.',\n",
    "}\n",
    "objs = [(str(k), Simhash(get_features(v))) for k, v in data.items()]\n",
    "index = SimhashIndex(objs, k=3)\n",
    "\n",
    "print(index.bucket_size())\n",
    "\n",
    "s1 = Simhash(get_features(u'How are you i am fine. blar blar blar blar blar thank'))\n",
    "print(index.get_near_dups(s1))\n",
    "\n",
    "index.add('4', s1)\n",
    "print(index.get_near_dups(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def lcs(seq1, seq2):\n",
    "    seq1 = seq1.split()\n",
    "    seq2 = seq2.split()\n",
    "    dp = np.zeros((len(seq1)+1, len(seq2)+1))\n",
    "    for i in range(len(seq1)):\n",
    "        for j in range(len(seq2)):\n",
    "            dp[i + 1][j + 1] = max(dp[i + 1][j], dp[i][j], dp[i][j + 1])\n",
    "            if seq1[i] == seq2[j]:\n",
    "                dp[i + 1][j + 1] = max(dp[i][j] + 1, dp[i + 1][j + 1])\n",
    "    seq1_mask = [ 0 for wd in seq1]\n",
    "    seq2_mask = [ 0 for wd in seq2]\n",
    "    ii, jj = len(seq1), len(seq2)\n",
    "    while ii != 0 and jj != 0:\n",
    "        if dp[ii][jj] == dp[ii - 1][jj - 1] + 1 and seq1[ii - 1] == seq2[jj - 1]:\n",
    "            seq1_mask[ii - 1] = 1\n",
    "            seq2_mask[jj - 1] = 1\n",
    "            ii = ii - 1\n",
    "            jj = jj - 1\n",
    "            continue\n",
    "        if dp[ii][jj] == dp[ii - 1][jj]:\n",
    "            ii = ii - 1\n",
    "        elif dp[ii][jj] == dp[ii][jj - 1]:\n",
    "            jj = jj - 1\n",
    "        elif dp[ii][jj] == dp[ii - 1][jj - 1]:\n",
    "            ii = ii - 1\n",
    "            jj = jj - 1\n",
    "    seq1_left = [ wd for wd, mk in zip(seq1, seq1_mask) if mk == 0]\n",
    "    seq2_left = [ wd for wd, mk in zip(seq2, seq2_mask) if mk == 0]\n",
    "    \n",
    "    return np.max(dp), ' '.join(seq1_left), ' '.join(seq2_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.0, 'kiss kk', 'fuck you')\n"
     ]
    }
   ],
   "source": [
    "print(lcs(\"kiss my ass kk\", \"fuck you my ass\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17553445 0.17553445 0.17553445 0.17553445 0.17553445 0.87767223\n",
      " 0.27500863]\n",
      "[0.17553445 0.17553445 0.17553445 0.17553445 0.17553445 0.87767223\n",
      " 0.27500863]\n",
      "[0.5 0.5 0.5 0.5]\n",
      "[0.17553445 0.17553445 0.17553445 0.17553445 0.17553445 0.87767223\n",
      " 0.27500863]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "data = [\n",
    "            'How are you? I Am fine. blar blar blar blar blar Thanks.',\n",
    "            'How are you i am fine. blar blar blar blar blar than',\n",
    "            'This is simhash test.',\n",
    "            'How are you i am fine. blar blar blar blar blar thank1'\n",
    "        ]\n",
    "vec = TfidfVectorizer()\n",
    "D = vec.fit_transform(data)\n",
    "voc = dict((i, w) for w, i in vec.vocabulary_.items())\n",
    "shs = []\n",
    "for Di in D:\n",
    "    print(Di.data)\n",
    "    # features as list of (token, weight) tuples)\n",
    "    features = zip([voc[j] for j in Di.indices], Di.data)\n",
    "    shs.append(Simhash(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSimhashIndex():\n",
    "    data = {\n",
    "        1: 'How are you? I Am fine. blar blar blar blar blar Thanks.',\n",
    "        2: 'How are you i am fine. blar blar blar blar blar than',\n",
    "        3: 'This is simhash test.',\n",
    "        4: 'How are you i am fine. blar blar blar blar blar thank1',\n",
    "    }\n",
    "\n",
    "    def setUp(self):\n",
    "        objs = [(str(k), Simhash(v)) for k, v in self.data.items()]\n",
    "        self.index = SimhashIndex(objs, k=10)\n",
    "\n",
    "    def test_get_near_dup(self):\n",
    "        s1 = Simhash(u'How are you i am fine.ablar ablar xyz blar blar blar blar blar blar blar thank')\n",
    "        dups = self.index.get_near_dups(s1)\n",
    "        print(dups)\n",
    "\n",
    "#         self.index.delete('1', Simhash(self.data[1]))\n",
    "#         dups = self.index.get_near_dups(s1)\n",
    "#         print(dups)\n",
    "\n",
    "#         self.index.delete('1', Simhash(self.data[1]))\n",
    "#         dups = self.index.get_near_dups(s1)\n",
    "#         self.assertEqual(len(dups), 2)\n",
    "\n",
    "#         self.index.add('1', Simhash(self.data[1]))\n",
    "#         dups = self.index.get_near_dups(s1)\n",
    "#         self.assertEqual(len(dups), 3)\n",
    "\n",
    "#         self.index.add('1', Simhash(self.data[1]))\n",
    "#         dups = self.index.get_near_dups(s1)\n",
    "#         self.assertEqual(len(dups), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '4', '2']\n"
     ]
    }
   ],
   "source": [
    "tmp = TestSimhashIndex()\n",
    "tmp.setUp()\n",
    "tmp.test_get_near_dup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'get_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-4b6fe93af699>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Challenges in natural language processing frequently involve speech recognition, natural language understanding, natural language\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'get_graph'"
     ]
    }
   ],
   "source": [
    "text = \"Challenges in natural language processing frequently involve speech recognition, natural language understanding, natural language\"\n",
    "print(keywords.get_graph(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"1427 5661 29788 1427 387 2299,\n",
    "372 22 1586 1025 218 165 218 27 7092 22 266 25817 4550 \n",
    "2136 11 60847 9156 1077 797 27 689 3447 11146 108 1667 46829 161702 1113017 548478 24274 5062 46829.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from summa import summarizer\n",
    "print(len(summarizer.summarize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<summa.graph.Graph object at 0x1108ba518>\n"
     ]
    }
   ],
   "source": [
    "from summa import keywords\n",
    "print(keywords.get_graph(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'get_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-7ec2e40c7f13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"12 32 44\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'get_graph'"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "print(keywords.get_graph(\"12 32 44\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
