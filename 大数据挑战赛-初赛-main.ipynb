{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pypi/simple\n",
      "Requirement already satisfied: hyperopt in /Users/niudong/workon_home/py3/lib/python3.6/site-packages (0.1.2)\n",
      "Requirement already satisfied: pymongo in /Users/niudong/workon_home/py3/lib/python3.6/site-packages (from hyperopt) (3.8.0)\n",
      "Requirement already satisfied: tqdm in /Users/niudong/workon_home/py3/lib/python3.6/site-packages (from hyperopt) (4.32.1)\n",
      "Requirement already satisfied: numpy in /Users/niudong/workon_home/py3/lib/python3.6/site-packages (from hyperopt) (1.16.2)\n",
      "Requirement already satisfied: six in /Users/niudong/workon_home/py3/lib/python3.6/site-packages (from hyperopt) (1.11.0)\n",
      "Requirement already satisfied: networkx in /Users/niudong/workon_home/py3/lib/python3.6/site-packages (from hyperopt) (1.8.1)\n",
      "Requirement already satisfied: future in /Users/niudong/workon_home/py3/lib/python3.6/site-packages (from hyperopt) (0.17.1)\n",
      "Requirement already satisfied: scipy in /Users/niudong/workon_home/py3/lib/python3.6/site-packages (from hyperopt) (1.1.0)\n",
      "Looking in indexes: http://pypi/simple\n",
      "Requirement already satisfied: pyemd in /Users/niudong/workon_home/py3/lib/python3.6/site-packages (0.5.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /Users/niudong/workon_home/py3/lib/python3.6/site-packages (from pyemd) (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install  --index \"http://pypi/simple\" --trusted-host pypi  hyperopt\n",
    "! pip install  --index \"http://pypi/simple\" --trusted-host pypi  pyemd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 包引入与变量准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "sys.path.append(\"./global\")\n",
    "from helper import Timer, usetime, OFFLINE, cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFLINE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始训练与测试数据位置\n",
    "if OFFLINE:\n",
    "    ori_train_file = \"./legacy/data/train.csv\"\n",
    "    ori_test_file = \"./legacy/data/test.csv\"\n",
    "    ori_test_label_file = \"./legacy/data/test_label.csv\"\n",
    "else:\n",
    "    ori_train_file = \"./legacy/data/train.csv\"\n",
    "    ori_test_file = \"./legacy/data/test.csv\"\n",
    "#     ori_train_file = \"/home/kesci/input/bytedance/first-round/train.csv\"\n",
    "#     ori_test_file = \"/home/kesci/input/bytedance/first-round/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage0阶段\n",
    "stage0_output_dir = \"./stage1/input/\"\n",
    "## 训练数据相关\n",
    "if OFFLINE:\n",
    "    train_file = stage0_output_dir + \"train.csv\"\n",
    "else:\n",
    "    sample_way = \"orderly_sample\"\n",
    "    if sample_way == \"random\":\n",
    "        random_rate = 0.05\n",
    "        train_file = stage0_output_dir + \"train_random_005.csv\"\n",
    "    elif sample_way == \"pre_fixed\":\n",
    "        sample_num = 5000000\n",
    "        # 默认取前500w条\n",
    "        train_file = stage0_output_dir + \"train_front_500w.csv\"\n",
    "    elif sample_way == \"orderly_sample\":\n",
    "        sample_rate = 0.05\n",
    "        train_file = stage0_output_dir + \"train_orderly_500w.csv\"\n",
    "## 测试数据相关\n",
    "test_file = stage0_output_dir + \"test.csv\"\n",
    "# 线下可以增加测试集label\n",
    "if OFFLINE:\n",
    "    test_label_file = stage0_output_dir + \"test_label.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage1阶段\n",
    "state1_dir = \"./stage1/\"\n",
    "stage1_code_dir = state1_dir + \"code/\"\n",
    "stage1_input_dir = state1_dir + \"input/\"\n",
    "stage1_output_dir = state1_dir + \"output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage2阶段\n",
    "state2_dir = \"./stage2/\"\n",
    "stage2_input_dir = state2_dir + \"input/\"\n",
    "stage2_output_dir = state2_dir + \"output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate_all = False  # 谨慎使用这个参数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage0阶段\n",
    "\n",
    "这一阶段主要是数据准备，且只需要运行一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stage0.code.data_prepare import RandomSampleCSV, SampleCSV, SampleWithRate\n",
    "regenerate_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'Sample with rate' block...\n",
      "processing chunk...\n",
      "processing chunk...\n",
      "processing chunk...\n",
      "processing chunk...\n",
      "Finished process.\n",
      "----->Finished 'Sample with rate' block, time used: 0.1s.\n"
     ]
    }
   ],
   "source": [
    "if regenerate_all or regenerate_data:\n",
    "    # 训练数据准备\n",
    "    if OFFLINE:\n",
    "        cp(ori_train_file, train_file)\n",
    "    else:\n",
    "        if sample_way == \"random\":\n",
    "            # 随机从一亿数据中采样rate比例的数据来训练\n",
    "            RandomSampleCSV(source_csv=ori_train_file, save_file=train_file, rate=random_rate)\n",
    "        elif sample_way == \"pre_fixed\":\n",
    "            # 取前c条数据来训练   \n",
    "            SampleCSV(source_csv=ori_train_file, save_file=train_file, count=sample_num)  \n",
    "        elif sample_way == \"orderly_sample\":\n",
    "            SampleWithRate(source_csv=ori_train_file, save_file=train_file, rate=sample_rate, chunk_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regenerate_all or regenerate_data:\n",
    "    # 测试数据准备\n",
    "    if OFFLINE:\n",
    "        cp(ori_test_file, test_file)\n",
    "        cp(ori_test_label_file, test_label_file)\n",
    "    else:\n",
    "        cp(ori_test_file, test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage1阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stage1.code.build_word2vec import PrepareWord2vecSamples, TrainWord2vec\n",
    "stage1_word2vec_sentences = stage1_input_dir+\"word2vec_sentences.txt\"\n",
    "stage1_word2vec_model = stage1_output_dir+\"word2vec\"\n",
    "regenerate_embedding_samples = False  # 重新生成适合embedding训练的样本集\n",
    "regenerate_embedding_model = False  # 重新生成适合embedding模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec\n",
    "\n",
    "1. 生成Word2vec可直接训练的sentences文件\n",
    "2. 训练word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成适合word2vec训练的样本集\n",
    "if regenerate_all or regenerate_embedding_samples:\n",
    "    PrepareWord2vecSamples(train_file, stage1_word2vec_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成word2vec模型\n",
    "if regenerate_all or regenerate_embedding_model:\n",
    "    TrainWord2vec(stage1_word2vec_sentences, stage1_word2vec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText\n",
    "\n",
    "FastText的使用：https://blog.csdn.net/ymaini/article/details/81489599\n",
    "\n",
    "1. 生成适合fastText训练的文本格式\n",
    "2. 有监督训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProcessForTrainFastText(train_file_data, stage1_input_dir+\"fastText_labeled_content.txt\", add_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd stage1/code; python build_fastText.py -f \"../input/fastText_labeled_content.txt\" -d \"../output/\" -s \"fastText_supervised.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stage1.code.feature_text import ExtractTextFeature\n",
    "from stage1.code.feature_embed import ExtractEmbedFeature\n",
    "from stage1.code.feature_vector_space import ExtractVectorSpaceFeature\n",
    "from helper import ORI_TRAIN_NAMES, ORI_TEST_NAMES, ORI_TRAIN_DTYPE, ORI_TEST_DTYPE\n",
    "feature_prefix = \"v1\"\n",
    "regenerate_feature = False\n",
    "regenerate_text_feature = False\n",
    "regenerate_embed_feature = False\n",
    "regenerate_vector_feature = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Mining Featues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regenerate_all or regenerate_feature or regenerate_text_feature:\n",
    "    # 提取train的特征\n",
    "    ExtractTextFeature(train_file, save_dir=stage1_output_dir, prefix=\"train_{}\".format(feature_prefix), \n",
    "                       names=ORI_TRAIN_NAMES, dtype=ORI_TRAIN_DTYPE, process_chunkly=False,\n",
    "                       drop_cols=['query', 'title', 'label', \"query_id\", \"query_title_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regenerate_all or regenerate_feature or regenerate_text_feature:\n",
    "    # 提取test的特征\n",
    "    ExtractTextFeature(test_file, save_dir=stage1_output_dir, prefix=\"test_{}\".format(feature_prefix), \n",
    "                       names=ORI_TEST_NAMES, dtype=ORI_TEST_DTYPE, process_chunkly=False,\n",
    "                       drop_cols=['query', 'title', \"query_id\", \"query_title_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regenerate_all or regenerate_feature or regenerate_embed_feature:\n",
    "    # 提取train的特征 \n",
    "    ExtractEmbedFeature(train_file, save_dir=stage1_output_dir, prefix=\"train_{}\".format(feature_prefix), \n",
    "                       names=ORI_TRAIN_NAMES, dtype=ORI_TRAIN_DTYPE, process_chunkly=False,\n",
    "                       drop_cols=['query', 'title', 'label', \"query_id\", \"query_title_id\"], \n",
    "                       embed_mode=\"word2vec\", embed_model_file=stage1_word2vec_model+\".kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regenerate_all or regenerate_feature or regenerate_embed_feature:\n",
    "    # 提取test的特征 \n",
    "    ExtractEmbedFeature(test_file, save_dir=stage1_output_dir, prefix=\"test_{}\".format(feature_prefix), \n",
    "                       names=ORI_TEST_NAMES, dtype=ORI_TEST_DTYPE, process_chunkly=False,\n",
    "                       drop_cols=['query', 'title', \"query_id\", \"query_title_id\"], \n",
    "                       embed_mode=\"word2vec\", embed_model_file=stage1_word2vec_model+\".kv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector space feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'Extract vector space feature' block...\n",
      "----->Started 'Extract one chunk feature' block...\n",
      "----->Started 'Fit tf-idf' block...\n",
      "----->Finished 'Fit tf-idf' block, time used: 0.13s.\n",
      "----->Started 'transform df1' block...\n",
      "----->Finished 'transform df1' block, time used: 0.25s.\n",
      "----->Started 'transform df2' block...\n",
      "----->Finished 'transform df2' block, time used: 0.22s.\n",
      "----->Finished 'Extract one chunk feature' block, time used: 0.71s.\n",
      "file saved to ./stage1/output/train_v1_feature_vector_space.csv\n",
      "----->Finished 'Extract vector space feature' block, time used: 0.77s.\n"
     ]
    }
   ],
   "source": [
    "if regenerate_all or regenerate_feature or regenerate_vector_feature:\n",
    "    # 提取train的特征 \n",
    "    ExtractVectorSpaceFeature(train_file, save_dir=stage1_output_dir, prefix=\"train_{}\".format(feature_prefix), \n",
    "                       names=ORI_TRAIN_NAMES, dtype=ORI_TRAIN_DTYPE, process_chunkly=False,\n",
    "                       drop_cols=['query', 'title', 'label', \"query_id\", \"query_title_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'Extract vector space feature' block...\n",
      "----->Started 'Extract one chunk feature' block...\n",
      "----->Started 'Fit tf-idf' block...\n",
      "----->Finished 'Fit tf-idf' block, time used: 0.81s.\n",
      "----->Started 'transform df1' block...\n",
      "----->Finished 'transform df1' block, time used: 1.77s.\n",
      "----->Started 'transform df2' block...\n",
      "----->Finished 'transform df2' block, time used: 2.05s.\n",
      "----->Finished 'Extract one chunk feature' block, time used: 5.11s.\n",
      "file saved to ./stage1/output/test_v1_feature_vector_space.csv\n",
      "----->Finished 'Extract vector space feature' block, time used: 5.18s.\n"
     ]
    }
   ],
   "source": [
    "if regenerate_all or regenerate_feature or regenerate_vector_feature:\n",
    "    # 提取test的特征\n",
    "    ExtractVectorSpaceFeature(test_file, save_dir=stage1_output_dir, prefix=\"test_{}\".format(feature_prefix), \n",
    "                       names=ORI_TEST_NAMES, dtype=ORI_TEST_DTYPE, process_chunkly=False,\n",
    "                       drop_cols=['query', 'title', \"query_id\", \"query_title_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for Stage2\n",
    "\n",
    "本节是将一些stage1产生的结果进过处理后放到stage2的input里面去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stage1.code.prepare_for_stage2 import ExtractTrainLabel, ConvertCSVToNPY, CombineFeatures\n",
    "\n",
    "# stage1\n",
    "## text feature\n",
    "train_text_feature_stage1 = stage1_output_dir + \"train_{}_feature_text.csv\".format(feature_prefix)\n",
    "test_text_feature_stage1 = stage1_output_dir + \"test_{}_feature_text.csv\".format(feature_prefix)\n",
    "## embed feature\n",
    "train_embed_feature_stage1 = stage1_output_dir + \"train_{}_feature_embed.csv\".format(feature_prefix)\n",
    "test_embed_feature_stage1 = stage1_output_dir + \"test_{}_feature_embed.csv\".format(feature_prefix)\n",
    "## embed feature\n",
    "train_vector_feature_stage1 = stage1_output_dir + \"train_{}_feature_vector_space.csv\".format(feature_prefix)\n",
    "test_vector_feature_stage1 = stage1_output_dir + \"test_{}_feature_vector_space.csv\".format(feature_prefix)\n",
    "## concat\n",
    "train_concat_feature_stage1 = stage1_output_dir + \"train_{}_feature_concat.csv\".format(feature_prefix)\n",
    "test_concat_feature_stage1 = stage1_output_dir + \"test_{}_feature_concat.csv\".format(feature_prefix)\n",
    "\n",
    "# stage2\n",
    "## text feature\n",
    "train_feature_stage2 = stage2_input_dir + \"train_{}_feature_concat.npy\".format(feature_prefix)\n",
    "test_feature_stage2 = stage2_input_dir + \"test_{}_feature_concat.npy\".format(feature_prefix)\n",
    "train_labels_stage2 = stage2_input_dir+\"train_labels.npy\"\n",
    "test_labels_stage2 = None\n",
    "if OFFLINE:\n",
    "    test_labels_stage2 = stage2_input_dir+\"test_labels.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'Extract label' block...\n",
      "Part of labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Label file saved to ./stage2/input/train_labels.npy\n",
      "----->Finished 'Extract label' block, time used: 0.0s.\n"
     ]
    }
   ],
   "source": [
    "# 提取标签\n",
    "if regenerate_all or regenerate_data:\n",
    "    # 提取训练集的标签并单独存储\n",
    "    ExtractTrainLabel(train_file, train_labels_stage2)\n",
    "    if OFFLINE:\n",
    "        # 提取测试集的标签并单独存储\n",
    "        ExtractTrainLabel(test_label_file, test_labels_stage2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----->Started 'Concat features' block...\n",
      "----->Finished 'Concat features' block, time used: 0.19s.\n",
      "----->Started 'Change inf to nan' block...\n",
      "----->Finished 'Change inf to nan' block, time used: 0.03s.\n",
      "----->Started 'Fill nan' block...\n",
      "----->Finished 'Fill nan' block, time used: 0.0s.\n",
      "----->Started 'Concat features' block...\n",
      "----->Finished 'Concat features' block, time used: 0.06s.\n",
      "----->Started 'Change inf to nan' block...\n",
      "----->Finished 'Change inf to nan' block, time used: 0.0s.\n",
      "----->Started 'Fill nan' block...\n",
      "----->Finished 'Fill nan' block, time used: 0.0s.\n",
      "----->Started 'Convert CSV To NPY' block...\n",
      "Part of rows: [[ 3.80000000e+01  1.80000000e+01  3.66356165e+00  2.94443898e+00\n",
      "   1.22448980e-01  6.52173913e-02  2.99206349e-01  4.48015873e-01\n",
      "   3.70000000e+01  1.70000000e+01  3.63758616e+00  2.89037176e+00\n",
      "   3.70370370e-02  1.88679245e-02  2.94723485e-01  3.85405710e-01\n",
      "   3.60000000e+01  1.60000000e+01  3.61091791e+00  2.83321334e+00\n",
      "   0.00000000e+00  0.00000000e+00  2.90305993e-01  3.48757608e-01\n",
      "   3.67024343e-02 -1.77346151e-01 -1.06448286e-01 -2.19657087e-01\n",
      "  -4.13007093e-01  9.72456928e-01  1.00000000e+00  6.18586492e+01\n",
      "   1.48718862e-01  1.31282658e-02  6.22528806e-03  2.37362464e-01\n",
      "   3.23770834e-02  1.04076160e+00]\n",
      " [ 3.80000000e+01  1.50000000e+01  3.66356165e+00  2.77258872e+00\n",
      "   2.50000000e-01  1.42857143e-01  3.26804432e-01  4.36208446e-01\n",
      "   3.70000000e+01  1.40000000e+01  3.63758616e+00  2.70805020e+00\n",
      "   7.84313725e-02  4.08163265e-02  3.22400584e-01  3.87520297e-01\n",
      "   3.60000000e+01  1.30000000e+01  3.61091791e+00  2.63905733e+00\n",
      "   0.00000000e+00  0.00000000e+00  3.09727853e-01  3.62881758e-01\n",
      "   2.51098717e-02 -1.77346151e-01 -1.64750234e-01 -2.19657087e-01\n",
      "  -2.49388403e-01  9.73393198e-01  1.00000000e+00  5.04022296e+01\n",
      "   7.15788495e-02  6.38107042e-03  3.09518024e-03  1.34064891e-01\n",
      "  -4.41352339e-02  1.07359213e+00]]\n",
      "Npy file saved to ./stage2/input/train_v1_feature_concat.npy\n",
      "----->Finished 'Convert CSV To NPY' block, time used: 0.19s.\n",
      "----->Started 'Convert CSV To NPY' block...\n",
      "Part of rows: [[ 4.00000000e+00  7.00000000e+00  1.60943791e+00  2.07944154e+00\n",
      "   5.45454545e-01  3.75000000e-01  5.45454545e-01  6.06060606e-01\n",
      "   3.00000000e+00  6.00000000e+00  1.38629436e+00  1.94591015e+00\n",
      "   4.44444444e-01  2.85714286e-01  4.44444444e-01  5.49019608e-01\n",
      "   2.00000000e+00  5.00000000e+00  1.09861229e+00  1.79175947e+00\n",
      "   2.85714286e-01  1.66666667e-01  3.91625616e-01  4.17582418e-01\n",
      "   1.43488831e-02 -5.87269951e-03 -8.81822548e-02 -2.48045844e-01\n",
      "  -8.92039257e-02  6.77448031e-01  1.00000000e+00  1.06185896e+02\n",
      "   8.59767598e-02  7.59695694e-03  3.63816338e-03  4.43416679e-01\n",
      "  -1.61268265e-01  1.18970057e-01]\n",
      " [ 3.00000000e+00  1.80000000e+01  1.38629436e+00  2.94443898e+00\n",
      "   3.00000000e-01  1.76470588e-01  2.85714286e-01  2.85714286e-01\n",
      "   2.00000000e+00  1.70000000e+01  1.09861229e+00  2.89037176e+00\n",
      "   2.10526316e-01  1.17647059e-01  2.10526316e-01  2.10526316e-01\n",
      "   1.00000000e+00  1.60000000e+01  6.93147181e-01  2.83321334e+00\n",
      "   1.17647059e-01  6.25000000e-02  1.17647059e-01  1.17647059e-01\n",
      "   2.86037602e-02 -2.40453449e-01 -1.83560107e-01 -3.02250229e-01\n",
      "  -2.78456662e-01  9.62260699e-01  1.00000000e+00  5.88345043e+01\n",
      "   9.14331785e-02  7.87450920e-03  3.70800924e-03  1.44180836e-01\n",
      "  -4.97595286e-02  9.17057517e-01]]\n",
      "Npy file saved to ./stage2/input/test_v1_feature_concat.npy\n",
      "----->Finished 'Convert CSV To NPY' block, time used: 0.09s.\n"
     ]
    }
   ],
   "source": [
    "# 拼接特征并输出到stage2\n",
    "if regenerate_all or regenerate_feature or regenerate_embed_feature or regenerate_text_feature or regenerate_vector_feature or True:\n",
    "    CombineFeatures([train_text_feature_stage1, train_embed_feature_stage1, train_vector_feature_stage1], \n",
    "                    train_concat_feature_stage1)\n",
    "    CombineFeatures([test_text_feature_stage1, test_embed_feature_stage1, test_vector_feature_stage1], \n",
    "                    test_concat_feature_stage1)\n",
    "    ConvertCSVToNPY(train_concat_feature_stage1, train_feature_stage2)\n",
    "    ConvertCSVToNPY(test_concat_feature_stage1, test_feature_stage2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 38)\n"
     ]
    }
   ],
   "source": [
    "tmp = np.load(train_feature_stage2)\n",
    "print(tmp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage2阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niudong/workon_home/py3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from stage2.code.run_hyperopt import LoadDataset, run_lgb_gbdt, run_lgb_dart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "[Running function]: _load_train\n",
      "[Used Time]: 0:00:00.004904\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "[Running function]: _load_test\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e5e4d649ab41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0min_train_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_labels_stage2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0min_test_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_feature_stage2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             in_test_label=test_labels_stage2)\n\u001b[0m",
      "\u001b[0;32m~/projects/bigDataContest-bytedance/stage2/code/run_hyperopt.py\u001b[0m in \u001b[0;36mLoadDataset\u001b[0;34m(in_train_feature, in_train_label, in_test_feature, in_test_label)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0m_load_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mOFFLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtest_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/bigDataContest-bytedance/global/helper.py\u001b[0m in \u001b[0;36m__usetime\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[Running function]: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mstime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mutime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[Used Time]: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/bigDataContest-bytedance/stage2/code/run_hyperopt.py\u001b[0m in \u001b[0;36m_load_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_test_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mOFFLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_test_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workon_home/py3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "LoadDataset(in_train_feature=train_feature_stage2, \n",
    "            in_train_label=train_labels_stage2, \n",
    "            in_test_feature=test_feature_stage2,\n",
    "            in_test_label=test_labels_stage2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lgb_gbdt(eval_num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_lgb_dart(eval_num=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
